"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[545],{8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>s});var a=i(6540);const t={},o=a.createContext(t);function r(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),a.createElement(o.Provider,{value:e},n.children)}},9809:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"ch10-vla-models/index","title":"Chapter 10: Vision-Language-Action Models","description":"This chapter covers Vision-Language-Action (VLA) models in robotics, which integrate visual perception, natural language understanding, and motor control in unified frameworks.","source":"@site/docs/ch10-vla-models/index.md","sourceDirName":"ch10-vla-models","slug":"/ch10-vla-models/","permalink":"/physical-ai-textbook/ch10-vla-models/","draft":false,"unlisted":false,"editUrl":"https://github.com/IlsaFatima1/physical-ai-textbook/tree/main/docs/ch10-vla-models/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"textbookSidebar","previous":{"title":"Chapter 9: Manipulation and Control Systems","permalink":"/physical-ai-textbook/ch09-manipulation/"},"next":{"title":"Chapter 11: Humanoid Robot Design and Control","permalink":"/physical-ai-textbook/ch11-humanoid-design/"}}');var t=i(4848),o=i(8453);const r={},s="Chapter 10: Vision-Language-Action Models",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Technical Explanation",id:"technical-explanation",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-10-vision-language-action-models",children:"Chapter 10: Vision-Language-Action Models"})}),"\n",(0,t.jsx)(e.p,{children:"This chapter covers Vision-Language-Action (VLA) models in robotics, which integrate visual perception, natural language understanding, and motor control in unified frameworks."}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the architecture of Vision-Language-Action models"}),"\n",(0,t.jsx)(e.li,{children:"Learn how VLA models enable multimodal reasoning in robotics"}),"\n",(0,t.jsx)(e.li,{children:"Explore practical applications of VLA in humanoid robotics"}),"\n",(0,t.jsx)(e.li,{children:"Implement basic VLA systems with ROS 2 and NVIDIA Isaac"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"technical-explanation",children:"Technical Explanation"}),"\n",(0,t.jsx)(e.p,{children:"Vision-Language-Action models represent a significant advancement in AI-integrated robotics, allowing robots to understand and respond to complex human instructions in natural language while perceiving and interacting with their environment. These models combine computer vision, natural language processing, and motor control in a unified architecture."}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"Vision-Language-Action models provide a powerful framework for creating more intuitive and capable robotic systems that can understand and respond to human commands in natural language while performing complex manipulation tasks."})]})}function u(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);