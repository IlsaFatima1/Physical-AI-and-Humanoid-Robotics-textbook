"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[668],{2664:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"ch07-perception-systems/index","title":"Chapter 7: Perception Systems and Computer Vision","description":"Learning Objectives","source":"@site/docs/ch07-perception-systems/index.md","sourceDirName":"ch07-perception-systems","slug":"/ch07-perception-systems/","permalink":"/physical-ai-textbook/ch07-perception-systems/","draft":false,"unlisted":false,"editUrl":"https://github.com/IlsaFatima1/physical-ai-textbook/tree/main/docs/ch07-perception-systems/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"textbookSidebar","previous":{"title":"Chapter 6: URDF and XACRO for Robot Modeling","permalink":"/physical-ai-textbook/ch06-urdf-xacro/"},"next":{"title":"Chapter 8: Mobile Robot Navigation and Path Planning","permalink":"/physical-ai-textbook/ch08-navigation/"}}');var i=r(4848),s=r(8453);const o={},a="Chapter 7: Perception Systems and Computer Vision",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"7.1 Introduction to Robot Perception",id:"71-introduction-to-robot-perception",level:2},{value:"7.1.1 The Perception-Action Loop",id:"711-the-perception-action-loop",level:3},{value:"7.1.2 Types of Perception in Robotics",id:"712-types-of-perception-in-robotics",level:3},{value:"7.2 Camera Systems and Image Acquisition",id:"72-camera-systems-and-image-acquisition",level:2},{value:"7.2.1 Camera Models and Calibration",id:"721-camera-models-and-calibration",level:3},{value:"7.2.2 Camera Calibration Process",id:"722-camera-calibration-process",level:3},{value:"7.2.3 Image Preprocessing",id:"723-image-preprocessing",level:3},{value:"7.3 Feature Detection and Matching",id:"73-feature-detection-and-matching",level:2},{value:"7.3.1 Key Point Detection",id:"731-key-point-detection",level:3},{value:"7.3.2 Image Alignment and Homography",id:"732-image-alignment-and-homography",level:3},{value:"7.4 Object Detection and Recognition",id:"74-object-detection-and-recognition",level:2},{value:"7.4.1 Traditional Object Detection",id:"741-traditional-object-detection",level:3},{value:"7.4.2 Deep Learning-Based Object Detection",id:"742-deep-learning-based-object-detection",level:3},{value:"7.5 ROS 2 Integration for Perception",id:"75-ros-2-integration-for-perception",level:2},{value:"7.5.1 Camera Data Processing Node",id:"751-camera-data-processing-node",level:3},{value:"7.5.2 Multi-Sensor Fusion",id:"752-multi-sensor-fusion",level:3},{value:"7.6 3D Perception and Reconstruction",id:"76-3d-perception-and-reconstruction",level:2},{value:"7.6.1 Stereo Vision",id:"761-stereo-vision",level:3},{value:"7.6.2 Structure from Motion (SfM)",id:"762-structure-from-motion-sfm",level:3},{value:"7.7 Deep Learning for Perception",id:"77-deep-learning-for-perception",level:2},{value:"7.7.1 Semantic Segmentation",id:"771-semantic-segmentation",level:3},{value:"7.8 Performance Evaluation and Uncertainty",id:"78-performance-evaluation-and-uncertainty",level:2},{value:"7.8.1 Perception Quality Metrics",id:"781-perception-quality-metrics",level:3},{value:"7.8.2 Uncertainty Quantification",id:"782-uncertainty-quantification",level:3},{value:"7.9 Integration with Physical AI Systems",id:"79-integration-with-physical-ai-systems",level:2},{value:"7.9.1 Robust Perception in Dynamic Environments",id:"791-robust-perception-in-dynamic-environments",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-7-perception-systems-and-computer-vision",children:"Chapter 7: Perception Systems and Computer Vision"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand the fundamental concepts of robot perception and computer vision"}),"\n",(0,i.jsx)(n.li,{children:"Implement various computer vision algorithms for robotics applications"}),"\n",(0,i.jsx)(n.li,{children:"Integrate perception systems with ROS 2 for real-time processing"}),"\n",(0,i.jsx)(n.li,{children:"Apply machine learning and deep learning techniques for object detection and recognition"}),"\n",(0,i.jsx)(n.li,{children:"Design perception pipelines for different robotic tasks and environments"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate perception system performance and handle uncertainty in sensor data"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"71-introduction-to-robot-perception",children:"7.1 Introduction to Robot Perception"}),"\n",(0,i.jsx)(n.p,{children:"Robot perception is the process by which robots acquire, interpret, and understand information about their environment. This capability is fundamental to autonomous robotics, enabling robots to navigate, manipulate objects, interact with humans, and perform complex tasks in unstructured environments."}),"\n",(0,i.jsx)(n.h3,{id:"711-the-perception-action-loop",children:"7.1.1 The Perception-Action Loop"}),"\n",(0,i.jsx)(n.p,{children:"In Physical AI systems, perception is tightly coupled with action through a continuous loop:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensing"}),": Acquiring data from various sensors (cameras, LiDAR, IMU, etc.)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Processing"}),": Interpreting sensor data to extract meaningful information"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Understanding"}),": Building a representation of the environment and objects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Decision Making"}),": Determining appropriate actions based on perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Acting"}),": Executing actions that may change the environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback"}),": Using the results of actions to refine perception"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"712-types-of-perception-in-robotics",children:"7.1.2 Types of Perception in Robotics"}),"\n",(0,i.jsx)(n.p,{children:"Robotics perception encompasses several modalities:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Visual Perception"}),": Processing camera images for object detection, recognition, and scene understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Range Perception"}),": Using LiDAR, depth sensors, or stereo vision for 3D scene reconstruction"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tactile Perception"}),": Sensing contact, force, and texture through touch"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Auditory Perception"}),": Processing sound for localization and recognition"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multimodal Perception"}),": Combining multiple sensory modalities for robust understanding"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"72-camera-systems-and-image-acquisition",children:"7.2 Camera Systems and Image Acquisition"}),"\n",(0,i.jsx)(n.h3,{id:"721-camera-models-and-calibration",children:"7.2.1 Camera Models and Calibration"}),"\n",(0,i.jsx)(n.p,{children:"Understanding camera models is crucial for accurate perception. The pinhole camera model is the foundation for most computer vision applications:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"u = fx * (X/Z) + cx\r\nv = fy * (Y/Z) + cy\n"})}),"\n",(0,i.jsx)(n.p,{children:"Where (u,v) are pixel coordinates, (X,Y,Z) are 3D world coordinates, and (fx,fy) are focal lengths, (cx,cy) are principal points."}),"\n",(0,i.jsx)(n.h3,{id:"722-camera-calibration-process",children:"7.2.2 Camera Calibration Process"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\n\r\ndef calibrate_camera(images, pattern_size=(9, 6)):\r\n    """Calibrate camera using chessboard pattern"""\r\n    # Prepare object points\r\n    objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)\r\n    objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)\r\n\r\n    # Arrays to store object points and image points\r\n    objpoints = []  # 3d points in real world space\r\n    imgpoints = []  # 2d points in image plane\r\n\r\n    for img in images:\r\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n\r\n        # Find chessboard corners\r\n        ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)\r\n\r\n        if ret:\r\n            objpoints.append(objp)\r\n            imgpoints.append(corners)\r\n\r\n    # Perform calibration\r\n    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\r\n        objpoints, imgpoints, gray.shape[::-1], None, None\r\n    )\r\n\r\n    return mtx, dist  # Camera matrix and distortion coefficients\n'})}),"\n",(0,i.jsx)(n.h3,{id:"723-image-preprocessing",children:"7.2.3 Image Preprocessing"}),"\n",(0,i.jsx)(n.p,{children:"Before processing, images often require preprocessing:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def preprocess_image(image, camera_matrix, dist_coeffs):\r\n    """Preprocess image for computer vision tasks"""\r\n    # Undistort image\r\n    h, w = image.shape[:2]\r\n    newcameramtx, roi = cv2.getOptimalNewCameraMatrix(\r\n        camera_matrix, dist_coeffs, (w, h), 1, (w, h)\r\n    )\r\n\r\n    undistorted = cv2.undistort(image, camera_matrix, dist_coeffs, None, newcameramtx)\r\n\r\n    # Apply ROI to remove black regions\r\n    x, y, w, h = roi\r\n    undistorted = undistorted[y:y+h, x:x+w]\r\n\r\n    # Convert to grayscale if needed\r\n    gray = cv2.cvtColor(undistorted, cv2.COLOR_BGR2GRAY)\r\n\r\n    return undistorted, gray\n'})}),"\n",(0,i.jsx)(n.h2,{id:"73-feature-detection-and-matching",children:"7.3 Feature Detection and Matching"}),"\n",(0,i.jsx)(n.h3,{id:"731-key-point-detection",children:"7.3.1 Key Point Detection"}),"\n",(0,i.jsx)(n.p,{children:"Feature detection is fundamental for many computer vision tasks:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def detect_features(image):\r\n    """Detect and describe features in an image"""\r\n    # Convert to grayscale\r\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n    # SIFT detector (requires opencv-contrib-python)\r\n    sift = cv2.SIFT_create()\r\n    keypoints, descriptors = sift.detectAndCompute(gray, None)\r\n\r\n    # Alternative: ORB detector (free alternative)\r\n    # orb = cv2.ORB_create()\r\n    # keypoints, descriptors = orb.detectAndCompute(gray, None)\r\n\r\n    return keypoints, descriptors\r\n\r\ndef match_features(desc1, desc2):\r\n    """Match features between two images"""\r\n    # Create BFMatcher object\r\n    bf = cv2.BFMatcher()\r\n\r\n    # Match descriptors\r\n    matches = bf.knnMatch(desc1, desc2, k=2)\r\n\r\n    # Apply ratio test to filter good matches\r\n    good_matches = []\r\n    for m, n in matches:\r\n        if m.distance < 0.75 * n.distance:\r\n            good_matches.append(m)\r\n\r\n    return good_matches\n'})}),"\n",(0,i.jsx)(n.h3,{id:"732-image-alignment-and-homography",children:"7.3.2 Image Alignment and Homography"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def find_homography(img1, img2):\r\n    """Find homography transformation between two images"""\r\n    kp1, desc1 = detect_features(img1)\r\n    kp2, desc2 = detect_features(img2)\r\n\r\n    matches = match_features(desc1, desc2)\r\n\r\n    if len(matches) >= 4:\r\n        # Get matched keypoint coordinates\r\n        src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\r\n        dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\r\n\r\n        # Find homography matrix\r\n        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\r\n\r\n        return H, matches, mask\r\n    else:\r\n        return None, [], None\n'})}),"\n",(0,i.jsx)(n.h2,{id:"74-object-detection-and-recognition",children:"7.4 Object Detection and Recognition"}),"\n",(0,i.jsx)(n.h3,{id:"741-traditional-object-detection",children:"7.4.1 Traditional Object Detection"}),"\n",(0,i.jsx)(n.p,{children:"Traditional methods use hand-crafted features:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def detect_objects_traditional(image):\r\n    """Detect objects using traditional computer vision methods"""\r\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n    # Edge detection\r\n    edges = cv2.Canny(gray, 50, 150)\r\n\r\n    # Find contours\r\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n    # Filter contours by area\r\n    min_area = 100\r\n    objects = []\r\n\r\n    for contour in contours:\r\n        area = cv2.contourArea(contour)\r\n        if area > min_area:\r\n            # Get bounding box\r\n            x, y, w, h = cv2.boundingRect(contour)\r\n            objects.append({\'bbox\': (x, y, w, h), \'area\': area})\r\n\r\n    return objects\r\n\r\ndef template_matching(image, template):\r\n    """Find template in image using template matching"""\r\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n    template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\r\n\r\n    # Perform template matching\r\n    result = cv2.matchTemplate(gray, template_gray, cv2.TM_CCOEFF_NORMED)\r\n\r\n    # Find locations where matching exceeds threshold\r\n    threshold = 0.8\r\n    locations = np.where(result >= threshold)\r\n\r\n    matches = []\r\n    for pt in zip(*locations[::-1]):\r\n        matches.append({\'bbox\': (pt[0], pt[1], template_gray.shape[1], template_gray.shape[0]),\r\n                        \'confidence\': result[pt[1], pt[0]]})\r\n\r\n    return matches\n'})}),"\n",(0,i.jsx)(n.h3,{id:"742-deep-learning-based-object-detection",children:"7.4.2 Deep Learning-Based Object Detection"}),"\n",(0,i.jsx)(n.p,{children:"Modern approaches use deep learning for superior performance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torchvision.transforms as transforms\r\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\r\n\r\nclass DeepObjectDetector:\r\n    def __init__(self, confidence_threshold=0.5):\r\n        self.confidence_threshold = confidence_threshold\r\n\r\n        # Load pre-trained model\r\n        self.model = fasterrcnn_resnet50_fpn(pretrained=True)\r\n        self.model.eval()\r\n\r\n        # Define image transforms\r\n        self.transform = transforms.Compose([\r\n            transforms.ToTensor()\r\n        ])\r\n\r\n    def detect_objects(self, image):\r\n        \"\"\"Detect objects using deep learning model\"\"\"\r\n        # Preprocess image\r\n        image_tensor = self.transform(image).unsqueeze(0)\r\n\r\n        # Perform inference\r\n        with torch.no_grad():\r\n            predictions = self.model(image_tensor)\r\n\r\n        # Extract results\r\n        boxes = predictions[0]['boxes'].cpu().numpy()\r\n        labels = predictions[0]['labels'].cpu().numpy()\r\n        scores = predictions[0]['scores'].cpu().numpy()\r\n\r\n        # Filter by confidence\r\n        detections = []\r\n        for i in range(len(boxes)):\r\n            if scores[i] > self.confidence_threshold:\r\n                detections.append({\r\n                    'bbox': boxes[i].astype(int),\r\n                    'label': int(labels[i]),\r\n                    'confidence': float(scores[i])\r\n                })\r\n\r\n        return detections\n"})}),"\n",(0,i.jsx)(n.h2,{id:"75-ros-2-integration-for-perception",children:"7.5 ROS 2 Integration for Perception"}),"\n",(0,i.jsx)(n.h3,{id:"751-camera-data-processing-node",children:"7.5.1 Camera Data Processing Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nfrom std_msgs.msg import String\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass PerceptionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'perception_node\')\r\n\r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n\r\n        # Object detector\r\n        self.detector = DeepObjectDetector()\r\n\r\n        # Setup subscribers and publishers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.detection_pub = self.create_publisher(\r\n            String,  # In practice, use a custom message type\r\n            \'/object_detections\',\r\n            10\r\n        )\r\n\r\n        # Setup processing timer\r\n        self.process_timer = self.create_timer(0.1, self.process_loop)\r\n\r\n        self.current_image = None\r\n        self.detections = []\r\n\r\n        self.get_logger().info("Perception node initialized")\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming camera image"""\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            self.current_image = cv_image\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error converting image: {e}\')\r\n\r\n    def process_loop(self):\r\n        """Main processing loop"""\r\n        if self.current_image is not None:\r\n            # Perform object detection\r\n            detections = self.detector.detect_objects(self.current_image)\r\n            self.detections = detections\r\n\r\n            # Process and publish results\r\n            if detections:\r\n                detection_info = f"Detected {len(detections)} objects"\r\n                detection_msg = String()\r\n                detection_msg.data = detection_info\r\n                self.detection_pub.publish(detection_msg)\r\n\r\n                # Visualize detections\r\n                self.visualize_detections()\r\n\r\n    def visualize_detections(self):\r\n        """Visualize detections on image"""\r\n        if self.current_image is not None and self.detections:\r\n            vis_image = self.current_image.copy()\r\n\r\n            for detection in self.detections:\r\n                bbox = detection[\'bbox\']\r\n                confidence = detection[\'confidence\']\r\n\r\n                # Draw bounding box\r\n                cv2.rectangle(vis_image,\r\n                             (int(bbox[0]), int(bbox[1])),\r\n                             (int(bbox[2]), int(bbox[3])),\r\n                             (0, 255, 0), 2)\r\n\r\n                # Add confidence text\r\n                cv2.putText(vis_image, f\'{confidence:.2f}\',\r\n                           (int(bbox[0]), int(bbox[1])-10),\r\n                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\r\n\r\n            # For visualization, you might publish the processed image\r\n            # processed_msg = self.cv_bridge.cv2_to_imgmsg(vis_image, "bgr8")\r\n            # self.processed_image_pub.publish(processed_msg)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"752-multi-sensor-fusion",children:"7.5.2 Multi-Sensor Fusion"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MultiSensorFusion(Node):\r\n    def __init__(self):\r\n        super().__init__(\'multi_sensor_fusion\')\r\n\r\n        # Subscribers for different sensors\r\n        self.camera_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.camera_callback, 10\r\n        )\r\n\r\n        self.lidar_sub = self.create_subscription(\r\n            PointCloud2, \'/lidar/points\', self.lidar_callback, 10\r\n        )\r\n\r\n        # Publisher for fused perception\r\n        self.fused_perception_pub = self.create_publisher(\r\n            PerceptionResult, \'/fused_perception\', 10\r\n        )\r\n\r\n        # Data storage\r\n        self.camera_data = None\r\n        self.lidar_data = None\r\n        self.camera_timestamp = None\r\n        self.lidar_timestamp = None\r\n\r\n    def camera_callback(self, msg):\r\n        """Process camera data"""\r\n        self.camera_data = msg\r\n        self.camera_timestamp = msg.header.stamp\r\n\r\n    def lidar_callback(self, msg):\r\n        """Process LiDAR data"""\r\n        self.lidar_data = msg\r\n        self.lidar_timestamp = msg.header.stamp\r\n\r\n    def fuse_sensor_data(self):\r\n        """Fuse camera and LiDAR data"""\r\n        if (self.camera_data is not None and\r\n            self.lidar_data is not None and\r\n            abs((self.camera_timestamp.sec + self.camera_timestamp.nanosec/1e9) -\r\n                (self.lidar_timestamp.sec + self.lidar_timestamp.nanosec/1e9)) < 0.1):\r\n\r\n            # Convert LiDAR points to camera frame\r\n            camera_image = self.cv_bridge.imgmsg_to_cv2(self.camera_data, "bgr8")\r\n            lidar_points = self.pointcloud2_to_array(self.lidar_data)\r\n\r\n            # Project 3D points to 2D image\r\n            projected_points = self.project_lidar_to_camera(lidar_points)\r\n\r\n            # Perform fusion (simplified example)\r\n            fused_result = self.perform_fusion(camera_image, projected_points)\r\n\r\n            # Publish result\r\n            self.publish_fused_result(fused_result)\r\n\r\n    def project_lidar_to_camera(self, lidar_points):\r\n        """Project LiDAR points to camera image coordinates"""\r\n        # This requires camera intrinsic and extrinsic parameters\r\n        # Simplified projection matrix\r\n        projection_matrix = np.array([\r\n            [1000, 0, 320],\r\n            [0, 1000, 240],\r\n            [0, 0, 1]\r\n        ])\r\n\r\n        # Project 3D points to 2D\r\n        projected = []\r\n        for point in lidar_points:\r\n            if point[2] > 0:  # Only points in front of camera\r\n                projected_point = projection_matrix @ np.array([point[0], point[1], point[2], 1])\r\n                projected_point = projected_point[:2] / projected_point[2]  # Normalize\r\n                projected.append(projected_point)\r\n\r\n        return projected\n'})}),"\n",(0,i.jsx)(n.h2,{id:"76-3d-perception-and-reconstruction",children:"7.6 3D Perception and Reconstruction"}),"\n",(0,i.jsx)(n.h3,{id:"761-stereo-vision",children:"7.6.1 Stereo Vision"}),"\n",(0,i.jsx)(n.p,{children:"Stereo vision enables depth estimation from two cameras:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class StereoProcessor:\r\n    def __init__(self, camera_config):\r\n        self.camera_config = camera_config\r\n\r\n        # Create stereo matcher\r\n        self.stereo = cv2.StereoSGBM_create(\r\n            minDisparity=0,\r\n            numDisparities=96,\r\n            blockSize=11,\r\n            P1=8 * 3 * 11**2,\r\n            P2=32 * 3 * 11**2,\r\n            disp12MaxDiff=1,\r\n            uniquenessRatio=15,\r\n            speckleWindowSize=0,\r\n            speckleRange=2,\r\n            preFilterCap=63,\r\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\r\n        )\r\n\r\n    def compute_disparity(self, left_image, right_image):\r\n        """Compute disparity map from stereo images"""\r\n        gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\r\n        gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\r\n\r\n        # Compute disparity\r\n        disparity = self.stereo.compute(gray_left, gray_right).astype(np.float32) / 16.0\r\n\r\n        return disparity\r\n\r\n    def reconstruct_3d(self, disparity, Q_matrix):\r\n        """Reconstruct 3D points from disparity"""\r\n        # Q is the reprojection matrix from stereo rectification\r\n        points_3d = cv2.reprojectImageTo3D(disparity, Q_matrix)\r\n\r\n        return points_3d\n'})}),"\n",(0,i.jsx)(n.h3,{id:"762-structure-from-motion-sfm",children:"7.6.2 Structure from Motion (SfM)"}),"\n",(0,i.jsx)(n.p,{children:"Structure from Motion reconstructs 3D scenes from multiple 2D images:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def structure_from_motion(images):\r\n    """Perform Structure from Motion reconstruction"""\r\n    # Extract features from all images\r\n    all_keypoints = []\r\n    all_descriptors = []\r\n\r\n    for img in images:\r\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n        kp, desc = detect_features(img)\r\n        all_keypoints.append(kp)\r\n        all_descriptors.append(desc)\r\n\r\n    # Match features between consecutive images\r\n    matches_sequence = []\r\n    for i in range(len(all_descriptors) - 1):\r\n        matches = match_features(all_descriptors[i], all_descriptors[i+1])\r\n        matches_sequence.append(matches)\r\n\r\n    # Estimate camera poses and 3D structure\r\n    # This is a simplified version - full SfM is more complex\r\n    camera_poses = []\r\n    points_3d = []\r\n\r\n    # Initialize with first camera at origin\r\n    camera_poses.append(np.eye(4))  # Identity matrix\r\n\r\n    for i, matches in enumerate(matches_sequence):\r\n        if len(matches) >= 8:  # Need at least 8 points for pose estimation\r\n            # Get matched points\r\n            src_pts = np.float32([all_keypoints[i][m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\r\n            dst_pts = np.float32([all_keypoints[i+1][m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\r\n\r\n            # Estimate essential matrix\r\n            E, mask = cv2.findEssentialMat(src_pts, dst_pts)\r\n\r\n            # Recover pose\r\n            if E is not None:\r\n                _, R, t, _ = cv2.recoverPose(E, src_pts, dst_pts)\r\n\r\n                # Create transformation matrix\r\n                pose = np.eye(4)\r\n                pose[:3, :3] = R\r\n                pose[:3, 3] = t.flatten()\r\n\r\n                camera_poses.append(pose)\r\n\r\n    return camera_poses, points_3d\n'})}),"\n",(0,i.jsx)(n.h2,{id:"77-deep-learning-for-perception",children:"7.7 Deep Learning for Perception"}),"\n",(0,i.jsx)(n.h3,{id:"771-semantic-segmentation",children:"7.7.1 Semantic Segmentation"}),"\n",(0,i.jsx)(n.p,{children:"Semantic segmentation provides pixel-level object classification:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as transforms\r\n\r\nclass SemanticSegmentation(nn.Module):\r\n    def __init__(self, num_classes=21):  # Pascal VOC has 21 classes\r\n        super(SemanticSegmentation, self).__init__()\r\n\r\n        # Using a simplified U-Net-like architecture\r\n        self.encoder = torch.hub.load(\'pytorch/vision:v0.10.0\',\r\n                                     \'resnet50\',\r\n                                     pretrained=True)\r\n\r\n        # Modify the final layer for segmentation\r\n        self.decoder = nn.Sequential(\r\n            nn.ConvTranspose2d(2048, 512, kernel_size=4, stride=2, padding=1),\r\n            nn.ReLU(inplace=True),\r\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\r\n            nn.ReLU(inplace=True),\r\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(128, num_classes, kernel_size=1)\r\n        )\r\n\r\n    def forward(self, x):\r\n        # Extract features using encoder\r\n        features = self.encoder(x)\r\n\r\n        # Decode features to segmentation map\r\n        segmentation = self.decoder(features)\r\n\r\n        return segmentation\r\n\r\nclass SegmentationNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'segmentation_node\')\r\n\r\n        self.cv_bridge = CvBridge()\r\n\r\n        # Load pre-trained segmentation model\r\n        self.model = SemanticSegmentation()\r\n        self.model.eval()\r\n\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10\r\n        )\r\n\r\n        self.segmentation_pub = self.create_publisher(Image, \'/segmentation\', 10)\r\n\r\n        # Define color map for visualization\r\n        self.color_map = self.create_color_map()\r\n\r\n    def image_callback(self, msg):\r\n        """Process image for semantic segmentation"""\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n\r\n            # Preprocess image\r\n            transform = transforms.Compose([\r\n                transforms.ToPILImage(),\r\n                transforms.Resize((480, 640)),\r\n                transforms.ToTensor(),\r\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                                   std=[0.229, 0.224, 0.225])\r\n            ])\r\n\r\n            input_tensor = transform(cv_image).unsqueeze(0)\r\n\r\n            # Perform segmentation\r\n            with torch.no_grad():\r\n                output = self.model(input_tensor)\r\n                predicted = torch.argmax(output, dim=1)\r\n\r\n            # Convert to color image for visualization\r\n            segmented_image = self.apply_color_map(predicted.squeeze().cpu().numpy())\r\n\r\n            # Publish result\r\n            result_msg = self.cv_bridge.cv2_to_imgmsg(segmented_image, "bgr8")\r\n            self.segmentation_pub.publish(result_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Segmentation error: {e}\')\r\n\r\n    def create_color_map(self):\r\n        """Create color map for different classes"""\r\n        # Pascal VOC color map\r\n        color_map = np.array([\r\n            [0, 0, 0],         # background\r\n            [128, 0, 0],       # aeroplane\r\n            [0, 128, 0],       # bicycle\r\n            [128, 128, 0],     # bird\r\n            [0, 0, 128],       # boat\r\n            [128, 0, 128],     # bottle\r\n            [0, 128, 128],     # bus\r\n            [128, 128, 128],   # car\r\n            [64, 0, 0],        # cat\r\n            [192, 0, 0],       # chair\r\n            [64, 128, 0],      # cow\r\n            [192, 128, 0],     # diningtable\r\n            [64, 0, 128],      # dog\r\n            [192, 0, 128],     # horse\r\n            [64, 128, 128],    # motorbike\r\n            [192, 128, 128],   # person\r\n            [0, 64, 0],        # potted plant\r\n            [128, 64, 0],      # sheep\r\n            [0, 192, 0],       # sofa\r\n            [128, 192, 0],     # train\r\n            [0, 64, 128]       # tv/monitor\r\n        ], dtype=np.uint8)\r\n\r\n        return color_map\r\n\r\n    def apply_color_map(self, segmentation_mask):\r\n        """Apply color map to segmentation mask"""\r\n        height, width = segmentation_mask.shape\r\n        colored_mask = np.zeros((height, width, 3), dtype=np.uint8)\r\n\r\n        for class_idx in np.unique(segmentation_mask):\r\n            mask = segmentation_mask == class_idx\r\n            colored_mask[mask] = self.color_map[class_idx]\r\n\r\n        return colored_mask\n'})}),"\n",(0,i.jsx)(n.h2,{id:"78-performance-evaluation-and-uncertainty",children:"7.8 Performance Evaluation and Uncertainty"}),"\n",(0,i.jsx)(n.h3,{id:"781-perception-quality-metrics",children:"7.8.1 Perception Quality Metrics"}),"\n",(0,i.jsx)(n.p,{children:"Evaluating perception system performance is crucial:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def evaluate_detection_performance(ground_truth, predictions, iou_threshold=0.5):\r\n    """Evaluate object detection performance"""\r\n    # Calculate IoU for each prediction-ground truth pair\r\n    ious = calculate_ious(ground_truth, predictions)\r\n\r\n    # Determine true positives, false positives, false negatives\r\n    tp = 0  # True positives\r\n    fp = 0  # False positives\r\n    fn = 0  # False negatives\r\n\r\n    matched_gt = set()\r\n\r\n    # For each prediction, find best matching ground truth\r\n    for pred_idx, pred in enumerate(predictions):\r\n        best_iou = 0\r\n        best_gt_idx = -1\r\n\r\n        for gt_idx, gt in enumerate(ground_truth):\r\n            if ious[pred_idx][gt_idx] > best_iou:\r\n                best_iou = ious[pred_idx][gt_idx]\r\n                best_gt_idx = gt_idx\r\n\r\n        if best_iou >= iou_threshold and best_gt_idx not in matched_gt:\r\n            tp += 1\r\n            matched_gt.add(best_gt_idx)\r\n        else:\r\n            fp += 1\r\n\r\n    fn = len(ground_truth) - len(matched_gt)\r\n\r\n    # Calculate metrics\r\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\r\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\r\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\r\n\r\n    return {\r\n        \'precision\': precision,\r\n        \'recall\': recall,\r\n        \'f1_score\': f1_score,\r\n        \'tp\': tp,\r\n        \'fp\': fp,\r\n        \'fn\': fn\r\n    }\r\n\r\ndef calculate_ious(boxes1, boxes2):\r\n    """Calculate IoU matrix between two sets of bounding boxes"""\r\n    ious = np.zeros((len(boxes1), len(boxes2)))\r\n\r\n    for i, box1 in enumerate(boxes1):\r\n        for j, box2 in enumerate(boxes2):\r\n            iou = calculate_iou(box1, box2)\r\n            ious[i, j] = iou\r\n\r\n    return ious\r\n\r\ndef calculate_iou(box1, box2):\r\n    """Calculate Intersection over Union for two bounding boxes"""\r\n    # Box format: [x1, y1, x2, y2]\r\n    x1 = max(box1[0], box2[0])\r\n    y1 = max(box1[1], box2[1])\r\n    x2 = min(box1[2], box2[2])\r\n    y2 = min(box1[3], box2[3])\r\n\r\n    if x2 < x1 or y2 < y1:\r\n        return 0.0\r\n\r\n    intersection = (x2 - x1) * (y2 - y1)\r\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\r\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\r\n    union = area1 + area2 - intersection\r\n\r\n    return intersection / union if union > 0 else 0.0\n'})}),"\n",(0,i.jsx)(n.h3,{id:"782-uncertainty-quantification",children:"7.8.2 Uncertainty Quantification"}),"\n",(0,i.jsx)(n.p,{children:"Quantifying uncertainty in perception results:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class UncertaintyEstimator:\r\n    def __init__(self):\r\n        self.uncertainty_models = {}\r\n\r\n    def estimate_detection_uncertainty(self, detection_result, input_image):\r\n        """Estimate uncertainty for object detection results"""\r\n        # Method 1: Monte Carlo Dropout\r\n        uncertainty_scores = self.monte_carlo_dropout_estimate(detection_result, input_image)\r\n\r\n        # Method 2: Ensemble prediction\r\n        ensemble_uncertainty = self.ensemble_estimate(detection_result, input_image)\r\n\r\n        # Combine uncertainties\r\n        final_uncertainty = 0.5 * uncertainty_scores + 0.5 * ensemble_uncertainty\r\n\r\n        return final_uncertainty\r\n\r\n    def monte_carlo_dropout_estimate(self, detection_result, input_image):\r\n        """Estimate uncertainty using Monte Carlo dropout"""\r\n        # This would involve running the model multiple times\r\n        # with dropout enabled during inference\r\n        pass\r\n\r\n    def ensemble_estimate(self, detection_result, input_image):\r\n        """Estimate uncertainty using model ensemble"""\r\n        # This would involve running multiple models and measuring disagreement\r\n        pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"79-integration-with-physical-ai-systems",children:"7.9 Integration with Physical AI Systems"}),"\n",(0,i.jsx)(n.p,{children:"Perception systems in Physical AI must handle real-world challenges:"}),"\n",(0,i.jsx)(n.h3,{id:"791-robust-perception-in-dynamic-environments",children:"7.9.1 Robust Perception in Dynamic Environments"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class RobustPerceptionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'robust_perception_node\')\r\n\r\n        # Adaptive parameters based on environment\r\n        self.lighting_condition = \'unknown\'\r\n        self.camera_motion = \'static\'\r\n\r\n        # Setup subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.adaptive_image_callback, 10\r\n        )\r\n\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, \'/imu/data\', self.imu_callback, 10\r\n        )\r\n\r\n    def adaptive_image_callback(self, msg):\r\n        """Adapt perception based on environmental conditions"""\r\n        cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n\r\n        # Assess lighting conditions\r\n        lighting_score = self.assess_lighting(cv_image)\r\n\r\n        # Adjust processing based on conditions\r\n        if lighting_score < 0.3:  # Low light\r\n            # Apply image enhancement\r\n            enhanced_image = self.enhance_low_light(cv_image)\r\n            # Use different detection parameters\r\n            detections = self.detect_with_enhancement(enhanced_image)\r\n        else:\r\n            detections = self.detect_objects(cv_image)\r\n\r\n        # If camera is moving, use motion compensation\r\n        if self.camera_motion == \'moving\':\r\n            detections = self.compensate_motion(detections)\r\n\r\n        # Publish results\r\n        self.publish_detections(detections)\r\n\r\n    def assess_lighting(self, image):\r\n        """Assess lighting conditions in image"""\r\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n        mean_brightness = np.mean(gray)\r\n        std_brightness = np.std(gray)\r\n\r\n        # Normalize to [0, 1] where 1 is good lighting\r\n        lighting_score = min(mean_brightness / 255.0, 1.0)\r\n\r\n        return lighting_score\r\n\r\n    def enhance_low_light(self, image):\r\n        """Enhance image for low-light conditions"""\r\n        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\r\n        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\r\n        l, a, b = cv2.split(lab)\r\n\r\n        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\r\n        l = clahe.apply(l)\r\n\r\n        enhanced_lab = cv2.merge([l, a, b])\r\n        enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\r\n\r\n        return enhanced_image\r\n\r\n    def imu_callback(self, msg):\r\n        """Update camera motion state from IMU"""\r\n        # Check if there\'s significant angular velocity\r\n        angular_velocity = np.linalg.norm([\r\n            msg.angular_velocity.x,\r\n            msg.angular_velocity.y,\r\n            msg.angular_velocity.z\r\n        ])\r\n\r\n        if angular_velocity > 0.1:  # Threshold for motion\r\n            self.camera_motion = \'moving\'\r\n        else:\r\n            self.camera_motion = \'static\'\n'})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Robot perception and computer vision form the sensory foundation of Physical AI systems. Modern perception systems combine traditional computer vision techniques with deep learning approaches to achieve robust performance across various environments and conditions. The integration with ROS 2 enables real-time processing and multi-sensor fusion, while uncertainty quantification ensures safe operation in uncertain environments. As robots become more autonomous, perception systems must continue to evolve to handle increasingly complex and dynamic real-world scenarios."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var t=r(6540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);