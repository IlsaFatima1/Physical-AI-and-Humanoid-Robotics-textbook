"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[153],{2293:(r,n,e)=>{e.r(n),e.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>_,frontMatter:()=>i,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"ch09-manipulation/index","title":"Chapter 9: Manipulation and Control Systems","description":"Learning Objectives","source":"@site/ch09-manipulation/index.md","sourceDirName":"ch09-manipulation","slug":"/ch09-manipulation/","permalink":"/ch09-manipulation/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/ch09-manipulation/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"textbookSidebar","previous":{"title":"Chapter 8: Mobile Robot Navigation and Path Planning","permalink":"/ch08-navigation/"},"next":{"title":"Chapter 10: Vision-Language-Action Models","permalink":"/ch10-vla-models/"}}');var o=e(4848),a=e(8453);const i={},s="Chapter 9: Manipulation and Control Systems",l={},p=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"9.1 Introduction to Robotic Manipulation",id:"91-introduction-to-robotic-manipulation",level:2},{value:"9.1.1 Manipulation System Components",id:"911-manipulation-system-components",level:3},{value:"9.1.2 Manipulation Challenges",id:"912-manipulation-challenges",level:3},{value:"9.2 Kinematics for Manipulation",id:"92-kinematics-for-manipulation",level:2},{value:"9.2.1 Forward Kinematics",id:"921-forward-kinematics",level:3},{value:"9.2.2 Inverse Kinematics",id:"922-inverse-kinematics",level:3},{value:"9.3 Control Systems for Manipulation",id:"93-control-systems-for-manipulation",level:2},{value:"9.3.1 PID Control for Joint Control",id:"931-pid-control-for-joint-control",level:3},{value:"9.3.2 Impedance Control",id:"932-impedance-control",level:3},{value:"9.3.3 Operational Space Control",id:"933-operational-space-control",level:3},{value:"9.4 Grasping and Grasp Planning",id:"94-grasping-and-grasp-planning",level:2},{value:"9.4.1 Grasp Representation",id:"941-grasp-representation",level:3},{value:"9.4.2 Grasp Planning Algorithms",id:"942-grasp-planning-algorithms",level:3},{value:"9.5 Manipulation Planning",id:"95-manipulation-planning",level:2},{value:"9.5.1 Task and Motion Planning",id:"951-task-and-motion-planning",level:3},{value:"9.5.2 Trajectory Generation and Smoothing",id:"952-trajectory-generation-and-smoothing",level:3},{value:"9.6 ROS 2 Integration for Manipulation",id:"96-ros-2-integration-for-manipulation",level:2},{value:"9.6.1 MoveIt! Integration",id:"961-moveit-integration",level:3},{value:"9.7 Perception-Action Integration",id:"97-perception-action-integration",level:2},{value:"9.7.1 Visual Servoing",id:"971-visual-servoing",level:3},{value:"9.8 Safety and Robustness",id:"98-safety-and-robustness",level:2},{value:"9.8.1 Force Control and Compliance",id:"981-force-control-and-compliance",level:3},{value:"9.9 Integration with Physical AI Systems",id:"99-integration-with-physical-ai-systems",level:2},{value:"9.9.1 Perception-Action Loop for Manipulation",id:"991-perception-action-loop-for-manipulation",level:3},{value:"Summary",id:"summary",level:2}];function c(r){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...r.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-9-manipulation-and-control-systems",children:"Chapter 9: Manipulation and Control Systems"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand the fundamental concepts of robotic manipulation and control"}),"\n",(0,o.jsx)(n.li,{children:"Implement kinematic models for robotic arms and manipulators"}),"\n",(0,o.jsx)(n.li,{children:"Design and implement various control strategies for manipulation tasks"}),"\n",(0,o.jsx)(n.li,{children:"Integrate perception systems with manipulation for object interaction"}),"\n",(0,o.jsx)(n.li,{children:"Apply grasping and manipulation planning algorithms"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate manipulation system performance and handle uncertainties"}),"\n",(0,o.jsx)(n.li,{children:"Design safe and robust manipulation systems for real-world applications"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"91-introduction-to-robotic-manipulation",children:"9.1 Introduction to Robotic Manipulation"}),"\n",(0,o.jsx)(n.p,{children:"Robotic manipulation refers to the ability of a robot to purposefully control objects in its environment. This encompasses a wide range of tasks from simple pick-and-place operations to complex assembly and interaction tasks. Manipulation systems typically involve robotic arms with end-effectors that can grasp, move, and manipulate objects."}),"\n",(0,o.jsx)(n.h3,{id:"911-manipulation-system-components",children:"9.1.1 Manipulation System Components"}),"\n",(0,o.jsx)(n.p,{children:"A typical robotic manipulation system consists of:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulator"}),": The mechanical structure (arm) with multiple degrees of freedom"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"End-effector"}),": The tool at the end of the arm (gripper, suction cup, etc.)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Actuators"}),": Motors or other devices that provide motion"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensors"}),": Cameras, force sensors, tactile sensors for feedback"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Controller"}),": Software and hardware that coordinates the system"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception System"}),": For identifying and localizing objects"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"912-manipulation-challenges",children:"9.1.2 Manipulation Challenges"}),"\n",(0,o.jsx)(n.p,{children:"Robotic manipulation faces several key challenges:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Uncertainty"}),": Uncertain object poses, friction, and dynamics"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Complexity"}),": High-dimensional configuration spaces"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Dexterity"}),": Need for fine motor control and precision"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety"}),": Ensuring safe interaction with objects and humans"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time Performance"}),": Fast response for dynamic tasks"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"92-kinematics-for-manipulation",children:"9.2 Kinematics for Manipulation"}),"\n",(0,o.jsx)(n.h3,{id:"921-forward-kinematics",children:"9.2.1 Forward Kinematics"}),"\n",(0,o.jsx)(n.p,{children:"Forward kinematics calculates the position and orientation of the end-effector given joint angles. For a robotic arm with n joints:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"T = A\u2081(\u03b8\u2081) \xd7 A\u2082(\u03b8\u2082) \xd7 ... \xd7 A\u2099(\u03b8\u2099)\n"})}),"\n",(0,o.jsx)(n.p,{children:"Where T is the transformation matrix representing end-effector pose, and A\u1d62(\u03b8\u1d62) are the transformation matrices for each joint."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nimport math\r\n\r\nclass ForwardKinematics:\r\n    def __init__(self, dh_parameters):\r\n        """\r\n        Initialize with Denavit-Hartenberg parameters\r\n        dh_parameters: list of [a, alpha, d, theta_offset] for each joint\r\n        """\r\n        self.dh_params = dh_parameters\r\n\r\n    def dh_transform(self, a, alpha, d, theta):\r\n        """Calculate Denavit-Hartenberg transformation matrix"""\r\n        return np.array([\r\n            [math.cos(theta), -math.sin(theta)*math.cos(alpha), math.sin(theta)*math.sin(alpha), a*math.cos(theta)],\r\n            [math.sin(theta), math.cos(theta)*math.cos(alpha), -math.cos(theta)*math.sin(alpha), a*math.sin(theta)],\r\n            [0, math.sin(alpha), math.cos(alpha), d],\r\n            [0, 0, 0, 1]\r\n        ])\r\n\r\n    def calculate_pose(self, joint_angles):\r\n        """Calculate end-effector pose given joint angles"""\r\n        if len(joint_angles) != len(self.dh_params):\r\n            raise ValueError("Number of joint angles must match number of joints")\r\n\r\n        # Start with identity transformation\r\n        transform = np.eye(4)\r\n\r\n        for i, (a, alpha, d, theta_offset) in enumerate(self.dh_params):\r\n            theta = joint_angles[i] + theta_offset\r\n            joint_transform = self.dh_transform(a, alpha, d, theta)\r\n            transform = transform @ joint_transform\r\n\r\n        return transform\r\n\r\n    def get_position(self, joint_angles):\r\n        """Get end-effector position from joint angles"""\r\n        transform = self.calculate_pose(joint_angles)\r\n        return transform[0:3, 3]  # Extract position vector\r\n\r\n    def get_orientation(self, joint_angles):\r\n        """Get end-effector orientation from joint angles"""\r\n        transform = self.calculate_pose(joint_angles)\r\n        return transform[0:3, 0:3]  # Extract rotation matrix\n'})}),"\n",(0,o.jsx)(n.h3,{id:"922-inverse-kinematics",children:"9.2.2 Inverse Kinematics"}),"\n",(0,o.jsx)(n.p,{children:"Inverse kinematics solves for joint angles given a desired end-effector pose. This is often more challenging than forward kinematics."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class InverseKinematics:\r\n    def __init__(self, dh_parameters, max_iterations=100, tolerance=1e-6):\r\n        self.dh_params = dh_parameters\r\n        self.max_iterations = max_iterations\r\n        self.tolerance = tolerance\r\n        self.forward_kinematics = ForwardKinematics(dh_parameters)\r\n\r\n    def jacobian(self, joint_angles):\r\n        """Calculate the Jacobian matrix for the manipulator"""\r\n        n = len(joint_angles)\r\n        jacobian = np.zeros((6, n))  # 6 DoF (3 position, 3 orientation)\r\n\r\n        # Calculate forward kinematics for all joint configurations\r\n        current_transform = np.eye(4)\r\n        joint_positions = [np.zeros(3)]  # Position of each joint\r\n\r\n        for i, (a, alpha, d, theta_offset) in enumerate(self.dh_params):\r\n            theta = joint_angles[i] + theta_offset\r\n            joint_transform = self.forward_kinematics.dh_transform(a, alpha, d, theta)\r\n            current_transform = current_transform @ joint_transform\r\n            joint_positions.append(current_transform[0:3, 3])\r\n\r\n        # End-effector position\r\n        end_effector_pos = current_transform[0:3, 3]\r\n\r\n        # Calculate Jacobian columns\r\n        for i in range(n):\r\n            # For revolute joints\r\n            z_axis = current_transform[0:3, 2]  # z-axis of joint frame\r\n            joint_pos = joint_positions[i]\r\n\r\n            # Linear velocity component\r\n            jacobian[0:3, i] = np.cross(z_axis, end_effector_pos - joint_pos)\r\n            # Angular velocity component\r\n            jacobian[3:6, i] = z_axis\r\n\r\n        return jacobian\r\n\r\n    def solve(self, target_pose, initial_guess):\r\n        """Solve inverse kinematics using Jacobian transpose method"""\r\n        current_angles = np.array(initial_guess)\r\n\r\n        for iteration in range(self.max_iterations):\r\n            # Calculate current pose\r\n            current_transform = self.forward_kinematics.calculate_pose(current_angles)\r\n            current_pos = current_transform[0:3, 3]\r\n            current_rot = current_transform[0:3, 0:3]\r\n\r\n            target_pos = target_pose[0:3, 3]\r\n            target_rot = target_pose[0:3, 0:3]\r\n\r\n            # Calculate position error\r\n            pos_error = target_pos - current_pos\r\n\r\n            # Calculate orientation error (simplified as angle-axis representation)\r\n            rot_error_matrix = target_rot @ current_rot.T - current_rot @ target_rot.T\r\n            angle_error = np.array([\r\n                rot_error_matrix[2, 1],\r\n                rot_error_matrix[0, 2],\r\n                rot_error_matrix[1, 0]\r\n            ])\r\n\r\n            # Combine position and orientation errors\r\n            error = np.concatenate([pos_error, angle_error])\r\n\r\n            # Check if error is within tolerance\r\n            if np.linalg.norm(error) < self.tolerance:\r\n                return current_angles\r\n\r\n            # Calculate Jacobian\r\n            jacobian = self.jacobian(current_angles)\r\n\r\n            # Update joint angles using Jacobian transpose method\r\n            # Note: For better convergence, use pseudo-inverse instead\r\n            delta_angles = np.linalg.pinv(jacobian) @ error * 0.1  # Learning rate\r\n            current_angles += delta_angles\r\n\r\n            # Apply joint limits (simplified)\r\n            current_angles = np.clip(current_angles, -np.pi, np.pi)\r\n\r\n        # Return best solution found\r\n        return current_angles\r\n\r\n    def solve_analytical_2r(self, target_x, target_y, l1, l2):\r\n        """\r\n        Analytical solution for 2R planar manipulator\r\n        l1, l2: link lengths\r\n        """\r\n        # Calculate distance to target\r\n        r = math.sqrt(target_x**2 + target_y**2)\r\n\r\n        # Check if target is reachable\r\n        if r > l1 + l2:\r\n            # Target is outside workspace\r\n            target_x = target_x * (l1 + l2) / r\r\n            target_y = target_y * (l1 + l2) / r\r\n            r = l1 + l2\r\n        elif r < abs(l1 - l2):\r\n            # Target is inside workspace\r\n            return None  # No solution\r\n\r\n        # Calculate joint angles\r\n        cos_theta2 = (r**2 - l1**2 - l2**2) / (2 * l1 * l2)\r\n        sin_theta2 = math.sqrt(1 - cos_theta2**2)\r\n        theta2 = math.atan2(sin_theta2, cos_theta2)\r\n\r\n        k1 = l1 + l2 * cos_theta2\r\n        k2 = l2 * sin_theta2\r\n        theta1 = math.atan2(target_y, target_x) - math.atan2(k2, k1)\r\n\r\n        return [theta1, theta2]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"93-control-systems-for-manipulation",children:"9.3 Control Systems for Manipulation"}),"\n",(0,o.jsx)(n.h3,{id:"931-pid-control-for-joint-control",children:"9.3.1 PID Control for Joint Control"}),"\n",(0,o.jsx)(n.p,{children:"PID (Proportional-Integral-Derivative) control is fundamental for precise joint control:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class PIDController:\r\n    def __init__(self, kp, ki, kd, dt=0.01):\r\n        self.kp = kp\r\n        self.ki = ki\r\n        self.kd = kd\r\n        self.dt = dt\r\n\r\n        self.previous_error = 0\r\n        self.integral = 0\r\n\r\n    def compute(self, setpoint, measured_value):\r\n        """Compute control output using PID"""\r\n        error = setpoint - measured_value\r\n\r\n        # Proportional term\r\n        p_term = self.kp * error\r\n\r\n        # Integral term\r\n        self.integral += error * self.dt\r\n        i_term = self.ki * self.integral\r\n\r\n        # Derivative term\r\n        derivative = (error - self.previous_error) / self.dt\r\n        d_term = self.kd * derivative\r\n\r\n        # Store error for next iteration\r\n        self.previous_error = error\r\n\r\n        # Calculate output\r\n        output = p_term + i_term + d_term\r\n\r\n        return output\r\n\r\nclass JointController:\r\n    def __init__(self, joint_limits=(-np.pi, np.pi)):\r\n        self.joint_limits = joint_limits\r\n        self.pid_controllers = {}\r\n\r\n    def add_joint(self, joint_name, kp, ki, kd):\r\n        """Add a PID controller for a specific joint"""\r\n        self.pid_controllers[joint_name] = PIDController(kp, ki, kd)\r\n\r\n    def compute_torques(self, joint_positions, joint_velocities, desired_positions, desired_velocities):\r\n        """Compute torques for all joints"""\r\n        torques = {}\r\n\r\n        for joint_name, pid in self.pid_controllers.items():\r\n            if joint_name in joint_positions and joint_name in desired_positions:\r\n                # Compute position error\r\n                pos_error = desired_positions[joint_name] - joint_positions[joint_name]\r\n\r\n                # Compute velocity error\r\n                vel_error = 0\r\n                if joint_name in joint_velocities and joint_name in desired_velocities:\r\n                    vel_error = desired_velocities[joint_name] - joint_velocities[joint_name]\r\n\r\n                # Use position error for P term, velocity error for D term\r\n                pos_control = pid.kp * pos_error\r\n                vel_control = pid.kd * vel_error\r\n\r\n                # Total control effort\r\n                control_effort = pos_control + vel_control\r\n\r\n                # Apply joint limits\r\n                control_effort = np.clip(control_effort,\r\n                                       self.joint_limits[0], self.joint_limits[1])\r\n\r\n                torques[joint_name] = control_effort\r\n\r\n        return torques\n'})}),"\n",(0,o.jsx)(n.h3,{id:"932-impedance-control",children:"9.3.2 Impedance Control"}),"\n",(0,o.jsx)(n.p,{children:"Impedance control allows the robot to behave like a spring-mass-damper system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ImpedanceController:\r\n    def __init__(self, mass, damping, stiffness, dt=0.01):\r\n        self.mass = mass\r\n        self.damping = damping\r\n        self.stiffness = stiffness\r\n        self.dt = dt\r\n\r\n        # State variables\r\n        self.position = np.zeros(3)\r\n        self.velocity = np.zeros(3)\r\n        self.acceleration = np.zeros(3)\r\n\r\n    def update(self, desired_position, external_force):\r\n        """Update impedance controller state"""\r\n        # Calculate position and velocity errors\r\n        pos_error = desired_position - self.position\r\n        vel_error = -self.velocity  # Assuming desired velocity is 0\r\n\r\n        # Calculate impedance force\r\n        spring_force = self.stiffness * pos_error\r\n        damper_force = self.damping * vel_error\r\n        inertia_force = self.mass * (-self.acceleration)  # Opposes acceleration\r\n\r\n        # Total force\r\n        total_force = spring_force + damper_force + external_force\r\n\r\n        # Calculate acceleration using F = ma\r\n        self.acceleration = total_force / self.mass\r\n\r\n        # Update velocity and position using numerical integration\r\n        self.velocity += self.acceleration * self.dt\r\n        self.position += self.velocity * self.dt\r\n\r\n        return self.position, self.velocity, self.acceleration\r\n\r\n    def set_mass(self, new_mass):\r\n        """Adjust mass parameter"""\r\n        self.mass = new_mass\r\n\r\n    def set_damping(self, new_damping):\r\n        """Adjust damping parameter"""\r\n        self.damping = new_damping\r\n\r\n    def set_stiffness(self, new_stiffness):\r\n        """Adjust stiffness parameter"""\r\n        self.stiffness = new_stiffness\n'})}),"\n",(0,o.jsx)(n.h3,{id:"933-operational-space-control",children:"9.3.3 Operational Space Control"}),"\n",(0,o.jsx)(n.p,{children:"Operational space control allows direct control in Cartesian space:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class OperationalSpaceController:\r\n    def __init__(self, robot_model):\r\n        self.robot_model = robot_model\r\n        self.gravity_compensation = True\r\n\r\n    def compute_cartesian_control(self, current_joints, desired_pose, desired_twist,\r\n                                  kp_pos=100, ki_pos=0, kd_pos=10,\r\n                                  kp_rot=100, ki_rot=0, kd_rot=10):\r\n        """\r\n        Compute control in operational space\r\n        """\r\n        # Calculate forward kinematics\r\n        current_pose = self.robot_model.forward_kinematics(current_joints)\r\n        current_pos = current_pose[0:3, 3]\r\n        current_rot = current_pose[0:3, 0:3]\r\n\r\n        # Calculate Jacobian\r\n        jacobian = self.robot_model.jacobian(current_joints)\r\n\r\n        # Calculate pose errors\r\n        pos_error = desired_pose[0:3, 3] - current_pos\r\n        rot_error = self.rotation_error(current_rot, desired_pose[0:3, 0:3])\r\n\r\n        # Calculate twist errors\r\n        current_twist = jacobian @ self.robot_model.joint_velocities\r\n        twist_error = desired_twist - current_twist\r\n\r\n        # Compute operational space forces\r\n        pos_force = kp_pos * pos_error + kd_pos * twist_error[0:3]\r\n        rot_force = kp_rot * rot_error + kd_rot * twist_error[3:6]\r\n\r\n        # Combine forces\r\n        operational_force = np.concatenate([pos_force, rot_force])\r\n\r\n        # Transform to joint space using transpose Jacobian\r\n        # For better results, use pseudo-inverse: tau = J.T @ F\r\n        joint_torques = jacobian.T @ operational_force\r\n\r\n        # Add gravity compensation if enabled\r\n        if self.gravity_compensation:\r\n            gravity_torques = self.robot_model.gravity_compensation(current_joints)\r\n            joint_torques += gravity_torques\r\n\r\n        return joint_torques\r\n\r\n    def rotation_error(self, current_rot, desired_rot):\r\n        """Calculate rotation error as axis-angle representation"""\r\n        rot_error_matrix = desired_rot @ current_rot.T\r\n        angle = math.acos(min(max((np.trace(rot_error_matrix) - 1) / 2, -1), 1))\r\n\r\n        if abs(angle) < 1e-6:\r\n            return np.zeros(3)\r\n\r\n        # Calculate axis of rotation\r\n        axis = np.array([\r\n            rot_error_matrix[2, 1] - rot_error_matrix[1, 2],\r\n            rot_error_matrix[0, 2] - rot_error_matrix[2, 0],\r\n            rot_error_matrix[1, 0] - rot_error_matrix[0, 1]\r\n        ]) / (2 * math.sin(angle))\r\n\r\n        return angle * axis\n'})}),"\n",(0,o.jsx)(n.h2,{id:"94-grasping-and-grasp-planning",children:"9.4 Grasping and Grasp Planning"}),"\n",(0,o.jsx)(n.h3,{id:"941-grasp-representation",children:"9.4.1 Grasp Representation"}),"\n",(0,o.jsx)(n.p,{children:"Grasps are typically represented by the position, orientation, and configuration of the end-effector:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class Grasp:\r\n    def __init__(self, position, orientation, approach_direction, grasp_width=0.05):\r\n        self.position = np.array(position)  # 3D position\r\n        self.orientation = np.array(orientation)  # 3D orientation (e.g., quaternion)\r\n        self.approach_direction = np.array(approach_direction)  # Approach direction\r\n        self.grasp_width = grasp_width  # Required gripper width\r\n        self.quality = 0.0  # Grasp quality metric\r\n        self.forces = []  # Expected forces at contact points\r\n\r\n    def to_transformation_matrix(self):\r\n        """Convert grasp to transformation matrix"""\r\n        # Create rotation matrix from orientation\r\n        # This is a simplified version - in practice, you\'d use quaternion to matrix conversion\r\n        rotation = self.orientation_to_rotation_matrix(self.orientation)\r\n\r\n        # Create transformation matrix\r\n        transform = np.eye(4)\r\n        transform[0:3, 0:3] = rotation\r\n        transform[0:3, 3] = self.position\r\n\r\n        return transform\r\n\r\n    def orientation_to_rotation_matrix(self, orientation):\r\n        """Convert orientation (quaternion) to rotation matrix"""\r\n        # Assuming orientation is a quaternion [x, y, z, w]\r\n        x, y, z, w = orientation\r\n\r\n        # Calculate rotation matrix from quaternion\r\n        rotation = np.array([\r\n            [1 - 2*(y**2 + z**2), 2*(x*y - w*z), 2*(x*z + w*y)],\r\n            [2*(x*y + w*z), 1 - 2*(x**2 + z**2), 2*(y*z - w*x)],\r\n            [2*(x*z - w*y), 2*(y*z + w*x), 1 - 2*(x**2 + y**2)]\r\n        ])\r\n\r\n        return rotation\r\n\r\n    def is_feasible(self, object_properties, robot_properties):\r\n        """Check if grasp is feasible given object and robot properties"""\r\n        # Check if grasp width is appropriate for object\r\n        if self.grasp_width > object_properties[\'max_width\']:\r\n            return False\r\n\r\n        # Check if approach direction is valid\r\n        # (e.g., not colliding with object support surface)\r\n\r\n        # Check if robot can reach the grasp pose\r\n        try:\r\n            ik_solution = robot_properties[\'ik_solver\'].solve(\r\n                self.to_transformation_matrix(),\r\n                robot_properties[\'initial_joints\']\r\n            )\r\n            if ik_solution is not None:\r\n                return True\r\n        except:\r\n            pass\r\n\r\n        return False\n'})}),"\n",(0,o.jsx)(n.h3,{id:"942-grasp-planning-algorithms",children:"9.4.2 Grasp Planning Algorithms"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class GraspPlanner:\r\n    def __init__(self, robot_model, gripper_model):\r\n        self.robot_model = robot_model\r\n        self.gripper_model = gripper_model\r\n        self.approach_directions = [\r\n            [0, 0, 1],   # From above\r\n            [0, 0, -1],  # From below\r\n            [1, 0, 0],   # From front\r\n            [-1, 0, 0],  # From back\r\n            [0, 1, 0],   # From left\r\n            [0, -1, 0]   # From right\r\n        ]\r\n\r\n    def plan_grasps(self, object_mesh, object_pose):\r\n        """Plan potential grasps for an object"""\r\n        grasps = []\r\n\r\n        # Sample points on the object surface\r\n        surface_points = self.sample_surface_points(object_mesh)\r\n\r\n        for point in surface_points:\r\n            # Calculate surface normal at this point\r\n            normal = self.calculate_surface_normal(object_mesh, point)\r\n\r\n            # Generate grasps for different approach directions\r\n            for approach in self.approach_directions:\r\n                # Check if approach direction is valid (not opposite to normal)\r\n                if np.dot(approach, normal) > 0.1:  # At least 10% alignment\r\n                    grasp = self.create_grasp(point, normal, approach)\r\n                    if grasp:\r\n                        # Evaluate grasp quality\r\n                        quality = self.evaluate_grasp_quality(grasp, object_mesh)\r\n                        grasp.quality = quality\r\n                        grasps.append(grasp)\r\n\r\n        # Sort grasps by quality\r\n        grasps.sort(key=lambda g: g.quality, reverse=True)\r\n\r\n        return grasps\r\n\r\n    def sample_surface_points(self, object_mesh):\r\n        """Sample points on the object surface"""\r\n        # This is a simplified approach\r\n        # In practice, you\'d use more sophisticated sampling methods\r\n        points = []\r\n\r\n        # For a simple box, sample corner points and face centers\r\n        # This is just an example - real implementation would depend on object representation\r\n        for x in [-0.1, 0, 0.1]:\r\n            for y in [-0.1, 0, 0.1]:\r\n                for z in [-0.1, 0, 0.1]:\r\n                    if abs(x) == 0.1 or abs(y) == 0.1 or abs(z) == 0.1:\r\n                        points.append([x, y, z])\r\n\r\n        return points\r\n\r\n    def calculate_surface_normal(self, object_mesh, point):\r\n        """Calculate surface normal at a point on the mesh"""\r\n        # Simplified normal calculation\r\n        # In practice, this would involve mesh processing\r\n        # For now, return a simple normal based on position\r\n        normal = np.array(point)\r\n        if np.linalg.norm(normal) > 0:\r\n            normal = normal / np.linalg.norm(normal)\r\n        else:\r\n            normal = np.array([0, 0, 1])\r\n\r\n        return normal\r\n\r\n    def create_grasp(self, position, normal, approach_direction):\r\n        """Create a grasp at the specified position and orientation"""\r\n        # Calculate orientation based on normal and approach direction\r\n        approach = np.array(approach_direction)\r\n        approach = approach / np.linalg.norm(approach)\r\n\r\n        normal = np.array(normal)\r\n        normal = normal / np.linalg.norm(normal)\r\n\r\n        # Calculate the orientation that aligns the gripper with the surface\r\n        # This is a simplified calculation\r\n        z_axis = -approach  # Gripper closing direction\r\n        x_axis = np.cross(normal, z_axis)\r\n        if np.linalg.norm(x_axis) < 0.1:  # Parallel vectors\r\n            x_axis = np.array([1, 0, 0])  # Default x-axis\r\n        else:\r\n            x_axis = x_axis / np.linalg.norm(x_axis)\r\n\r\n        y_axis = np.cross(z_axis, x_axis)\r\n        y_axis = y_axis / np.linalg.norm(y_axis)\r\n\r\n        # Create rotation matrix\r\n        rotation_matrix = np.column_stack([x_axis, y_axis, z_axis])\r\n\r\n        # Convert to quaternion (simplified)\r\n        # In practice, use proper quaternion conversion\r\n        quat = self.rotation_matrix_to_quaternion(rotation_matrix)\r\n\r\n        # Create grasp\r\n        grasp = Grasp(position, quat, approach_direction)\r\n\r\n        return grasp\r\n\r\n    def rotation_matrix_to_quaternion(self, rotation_matrix):\r\n        """Convert rotation matrix to quaternion"""\r\n        trace = np.trace(rotation_matrix)\r\n\r\n        if trace > 0:\r\n            s = math.sqrt(trace + 1.0) * 2  # S=4*qw\r\n            qw = 0.25 * s\r\n            qx = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\r\n            qy = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\r\n            qz = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\r\n        else:\r\n            if rotation_matrix[0, 0] > rotation_matrix[1, 1] and rotation_matrix[0, 0] > rotation_matrix[2, 2]:\r\n                s = math.sqrt(1.0 + rotation_matrix[0, 0] - rotation_matrix[1, 1] - rotation_matrix[2, 2]) * 2\r\n                qw = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\r\n                qx = 0.25 * s\r\n                qy = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\r\n                qz = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\r\n            elif rotation_matrix[1, 1] > rotation_matrix[2, 2]:\r\n                s = math.sqrt(1.0 + rotation_matrix[1, 1] - rotation_matrix[0, 0] - rotation_matrix[2, 2]) * 2\r\n                qw = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\r\n                qx = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\r\n                qy = 0.25 * s\r\n                qz = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\r\n            else:\r\n                s = math.sqrt(1.0 + rotation_matrix[2, 2] - rotation_matrix[0, 0] - rotation_matrix[1, 1]) * 2\r\n                qw = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\r\n                qx = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\r\n                qy = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\r\n                qz = 0.25 * s\r\n\r\n        return np.array([qx, qy, qz, qw])\r\n\r\n    def evaluate_grasp_quality(self, grasp, object_mesh):\r\n        """Evaluate the quality of a grasp"""\r\n        # This is a simplified quality evaluation\r\n        # In practice, this would involve more complex physics simulation\r\n\r\n        quality = 0.0\r\n\r\n        # Factors affecting grasp quality:\r\n        # 1. Force closure (ability to resist external forces)\r\n        # 2. Grasp stability\r\n        # 3. Accessibility (can robot reach the grasp?)\r\n        # 4. Object properties (friction, weight, etc.)\r\n\r\n        # For now, use a simple heuristic based on approach direction\r\n        # and surface normal alignment\r\n        approach = grasp.approach_direction\r\n        # Calculate surface normal at grasp position (simplified)\r\n        normal = np.array(grasp.orientation[0:3])  # Simplified\r\n\r\n        # Prefer grasps where approach direction is aligned with normal\r\n        alignment = abs(np.dot(approach, normal))\r\n        quality += alignment * 0.3\r\n\r\n        # Prefer grasps at the top of objects\r\n        if grasp.position[2] > 0.05:  # Above some threshold\r\n            quality += 0.2\r\n\r\n        # Prefer center grasps\r\n        center_dist = np.linalg.norm(grasp.position[0:2])\r\n        if center_dist < 0.05:  # Close to center\r\n            quality += 0.2\r\n\r\n        # Add random factor for diversity\r\n        quality += np.random.random() * 0.3\r\n\r\n        return min(quality, 1.0)  # Clamp to [0, 1]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"95-manipulation-planning",children:"9.5 Manipulation Planning"}),"\n",(0,o.jsx)(n.h3,{id:"951-task-and-motion-planning",children:"9.5.1 Task and Motion Planning"}),"\n",(0,o.jsx)(n.p,{children:"Task and Motion Planning (TAMP) combines high-level task planning with low-level motion planning:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class TaskAndMotionPlanner:\r\n    def __init__(self, robot_model, scene_objects):\r\n        self.robot_model = robot_model\r\n        self.scene_objects = scene_objects\r\n        self.motion_planner = RRTPlanner(robot_model)\r\n        self.task_planner = TaskPlanner()\r\n\r\n    def plan_manipulation_task(self, task_description):\r\n        """Plan a manipulation task combining task and motion planning"""\r\n        # Parse task description (e.g., "pick object A and place it at B")\r\n        task_plan = self.task_planner.plan_task(task_description)\r\n\r\n        full_plan = []\r\n        for task_step in task_plan:\r\n            motion_plan = self.plan_motion_for_task(task_step)\r\n            if motion_plan:\r\n                full_plan.extend(motion_plan)\r\n            else:\r\n                # Try alternative task plan\r\n                alternative_task_plan = self.task_planner.generate_alternative(task_step)\r\n                if alternative_task_plan:\r\n                    motion_plan = self.plan_motion_for_task(alternative_task_plan)\r\n                    if motion_plan:\r\n                        full_plan.extend(motion_plan)\r\n                    else:\r\n                        return None  # Cannot complete task\r\n                else:\r\n                    return None  # No alternative available\r\n\r\n        return full_plan\r\n\r\n    def plan_motion_for_task(self, task_step):\r\n        """Plan motion for a specific task step"""\r\n        if task_step[\'action\'] == \'pick\':\r\n            return self.plan_pick_motion(task_step)\r\n        elif task_step[\'action\'] == \'place\':\r\n            return self.plan_place_motion(task_step)\r\n        elif task_step[\'action\'] == \'move_to\':\r\n            return self.plan_navigation_motion(task_step)\r\n        else:\r\n            return None\r\n\r\n    def plan_pick_motion(self, task_step):\r\n        """Plan motion for picking an object"""\r\n        object_name = task_step[\'object\']\r\n        object_pose = self.scene_objects[object_name][\'pose\']\r\n\r\n        # Find a feasible grasp for the object\r\n        grasp_planner = GraspPlanner(self.robot_model, self.gripper_model)\r\n        grasps = grasp_planner.plan_grasps(\r\n            self.scene_objects[object_name][\'mesh\'],\r\n            object_pose\r\n        )\r\n\r\n        # For each grasp, plan a path to reach it\r\n        for grasp in grasps:\r\n            # Plan approach path\r\n            approach_pose = self.calculate_approach_pose(grasp)\r\n            approach_path = self.motion_planner.plan_path(\r\n                self.robot_model.get_current_pose(),\r\n                approach_pose\r\n            )\r\n\r\n            if approach_path:\r\n                # Plan grasp execution path\r\n                grasp_path = [grasp.to_transformation_matrix()]\r\n\r\n                # Plan lift path after grasp\r\n                lift_pose = self.calculate_lift_pose(grasp)\r\n                lift_path = self.motion_planner.plan_path(\r\n                    grasp.to_transformation_matrix(),\r\n                    lift_pose\r\n                )\r\n\r\n                if lift_path:\r\n                    return approach_path + grasp_path + lift_path\r\n\r\n        return None  # No feasible grasp found\r\n\r\n    def calculate_approach_pose(self, grasp, distance=0.1):\r\n        """Calculate approach pose before grasp"""\r\n        grasp_transform = grasp.to_transformation_matrix()\r\n        approach_transform = grasp_transform.copy()\r\n\r\n        # Move back along approach direction\r\n        approach_direction = grasp.approach_direction\r\n        approach_transform[0:3, 3] += distance * np.array(approach_direction)\r\n\r\n        return approach_transform\r\n\r\n    def calculate_lift_pose(self, grasp, height=0.1):\r\n        """Calculate lift pose after grasp"""\r\n        grasp_transform = grasp.to_transformation_matrix()\r\n        lift_transform = grasp_transform.copy()\r\n\r\n        # Move up along z-axis\r\n        lift_transform[0:3, 3][2] += height\r\n\r\n        return lift_transform\r\n\r\n    def plan_place_motion(self, task_step):\r\n        """Plan motion for placing an object"""\r\n        target_pose = task_step[\'target_pose\']\r\n\r\n        # Plan a path to the placement location\r\n        place_path = self.motion_planner.plan_path(\r\n            self.robot_model.get_current_pose(),\r\n            target_pose\r\n        )\r\n\r\n        if place_path:\r\n            # Add release action at the end\r\n            place_path.append((\'release\', {}))\r\n\r\n        return place_path\n'})}),"\n",(0,o.jsx)(n.h3,{id:"952-trajectory-generation-and-smoothing",children:"9.5.2 Trajectory Generation and Smoothing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class TrajectoryGenerator:\r\n    def __init__(self):\r\n        self.max_velocity = 1.0\r\n        self.max_acceleration = 0.5\r\n        self.dt = 0.01\r\n\r\n    def generate_minimal_jerk_trajectory(self, start_pos, end_pos, duration):\r\n        \"\"\"Generate a minimal jerk trajectory between two points\"\"\"\r\n        t = np.linspace(0, duration, int(duration / self.dt))\r\n        trajectory = []\r\n\r\n        # Minimal jerk trajectory coefficients\r\n        a0 = start_pos\r\n        a1 = 0  # Start velocity = 0\r\n        a2 = 0  # Start acceleration = 0\r\n        a3 = 10 * (end_pos - start_pos) / (duration ** 3)\r\n        a4 = -15 * (end_pos - start_pos) / (duration ** 4)\r\n        a5 = 6 * (end_pos - start_pos) / (duration ** 5)\r\n\r\n        for ti in t:\r\n            pos = a0 + a1*ti + a2*ti**2 + a3*ti**3 + a4*ti**4 + a5*ti**5\r\n            vel = a1 + 2*a2*ti + 3*a3*ti**2 + 4*a4*ti**3 + 5*a5*ti**4\r\n            acc = 2*a2 + 6*a3*ti + 12*a4*ti**2 + 20*a5*ti**3\r\n\r\n            trajectory.append({\r\n                'position': pos,\r\n                'velocity': vel,\r\n                'acceleration': acc,\r\n                'time': ti\r\n            })\r\n\r\n        return trajectory\r\n\r\n    def smooth_path(self, path, max_deviation=0.05):\r\n        \"\"\"Smooth a path using cubic spline interpolation\"\"\"\r\n        if len(path) < 3:\r\n            return path\r\n\r\n        # Convert path to numpy array for processing\r\n        path_array = np.array(path)\r\n\r\n        # Use cubic spline to smooth the path\r\n        from scipy.interpolate import CubicSpline\r\n\r\n        # Create parameter for interpolation\r\n        t = np.linspace(0, 1, len(path_array))\r\n\r\n        # Create spline for each dimension\r\n        cs = CubicSpline(t, path_array)\r\n\r\n        # Generate smooth path\r\n        t_new = np.linspace(0, 1, len(path_array) * 3)  # Increase resolution\r\n        smooth_path = cs(t_new)\r\n\r\n        return smooth_path.tolist()\r\n\r\n    def velocity_smoothing(self, trajectory, max_velocity=None, max_acceleration=None):\r\n        \"\"\"Apply velocity and acceleration limits to trajectory\"\"\"\r\n        if max_velocity is None:\r\n            max_velocity = self.max_velocity\r\n        if max_acceleration is None:\r\n            max_acceleration = self.max_acceleration\r\n\r\n        smoothed_trajectory = []\r\n        prev_vel = 0\r\n\r\n        for point in trajectory:\r\n            pos = point['position']\r\n            vel = np.clip(point['velocity'], -max_velocity, max_velocity)\r\n            acc = np.clip(point['acceleration'], -max_acceleration, max_acceleration)\r\n\r\n            # Limit acceleration based on previous velocity\r\n            max_change = max_acceleration * self.dt\r\n            vel = np.clip(vel, prev_vel - max_change, prev_vel + max_change)\r\n\r\n            smoothed_trajectory.append({\r\n                'position': pos,\r\n                'velocity': vel,\r\n                'acceleration': acc,\r\n                'time': point['time']\r\n            })\r\n\r\n            prev_vel = vel\r\n\r\n        return smoothed_trajectory\n"})}),"\n",(0,o.jsx)(n.h2,{id:"96-ros-2-integration-for-manipulation",children:"9.6 ROS 2 Integration for Manipulation"}),"\n",(0,o.jsx)(n.h3,{id:"961-moveit-integration",children:"9.6.1 MoveIt! Integration"}),"\n",(0,o.jsx)(n.p,{children:"MoveIt! is the standard motion planning framework for ROS:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom moveit_msgs.msg import MoveItErrorCodes\r\nfrom moveit_msgs.srv import GetPositionIK, GetPositionFK\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom sensor_msgs.msg import JointState\r\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\r\nfrom std_msgs.msg import Header\r\nimport numpy as np\r\n\r\nclass ManipulationControllerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'manipulation_controller\')\r\n\r\n        # Initialize MoveIt! interface\r\n        self.move_group = None\r\n        self.joint_state = None\r\n        self.end_effector_pose = None\r\n\r\n        # Setup subscribers and publishers\r\n        self.joint_state_sub = self.create_subscription(\r\n            JointState, \'/joint_states\', self.joint_state_callback, 10\r\n        )\r\n\r\n        self.command_pub = self.create_publisher(\r\n            JointTrajectory, \'/joint_trajectory_controller/joint_trajectory\', 10\r\n        )\r\n\r\n        # Setup manipulation services\r\n        self.ik_service = self.create_client(\r\n            GetPositionIK, \'/compute_ik\'\r\n        )\r\n\r\n        self.fk_service = self.create_client(\r\n            GetPositionFK, \'/compute_fk\'\r\n        )\r\n\r\n        # Setup timers for control loop\r\n        self.control_timer = self.create_timer(0.05, self.control_loop)\r\n\r\n        self.get_logger().info("Manipulation Controller initialized")\r\n\r\n    def joint_state_callback(self, msg):\r\n        """Update joint state"""\r\n        self.joint_state = msg\r\n\r\n    def move_to_pose(self, target_pose):\r\n        """Move end-effector to target pose using inverse kinematics"""\r\n        if not self.ik_service.wait_for_service(timeout_sec=1.0):\r\n            self.get_logger().error(\'IK service not available\')\r\n            return False\r\n\r\n        # Create IK request\r\n        ik_request = GetPositionIK.Request()\r\n        ik_request.ik_request.group_name = "manipulator"  # Your move group name\r\n        ik_request.ik_request.pose_stamped = target_pose\r\n        ik_request.ik_request.timeout.sec = 5\r\n        ik_request.ik_request.avoid_collisions = True\r\n\r\n        # Call IK service\r\n        future = self.ik_service.call_async(ik_request)\r\n        rclpy.spin_until_future_complete(self, future)\r\n\r\n        if future.result() is not None:\r\n            result = future.result()\r\n            if result.error_code.val == MoveItErrorCodes.SUCCESS:\r\n                # Execute trajectory\r\n                joint_trajectory = self.create_trajectory_from_ik_result(result.solution)\r\n                self.execute_trajectory(joint_trajectory)\r\n                return True\r\n            else:\r\n                self.get_logger().error(f\'IK failed with error: {result.error_code.val}\')\r\n                return False\r\n        else:\r\n            self.get_logger().error(\'Failed to call IK service\')\r\n            return False\r\n\r\n    def create_trajectory_from_ik_result(self, ik_solution):\r\n        """Create joint trajectory from IK solution"""\r\n        trajectory = JointTrajectory()\r\n        trajectory.joint_names = ik_solution.joint_state.name\r\n        trajectory.points = []\r\n\r\n        # Create trajectory point\r\n        point = JointTrajectoryPoint()\r\n        point.positions = ik_solution.joint_state.position\r\n        point.velocities = [0.0] * len(ik_solution.joint_state.position)\r\n        point.accelerations = [0.0] * len(ik_solution.joint_state.position)\r\n        point.time_from_start.sec = 3  # 3 seconds to reach target\r\n        point.time_from_start.nanosec = 0\r\n\r\n        trajectory.points.append(point)\r\n        return trajectory\r\n\r\n    def execute_trajectory(self, trajectory):\r\n        """Execute joint trajectory"""\r\n        self.command_pub.publish(trajectory)\r\n\r\n    def pick_object(self, object_pose, approach_height=0.1, lift_height=0.1):\r\n        """Execute pick operation"""\r\n        # 1. Move to approach position above object\r\n        approach_pose = PoseStamped()\r\n        approach_pose.pose = object_pose\r\n        approach_pose.pose.position.z += approach_height\r\n        approach_pose.header.frame_id = "base_link"\r\n\r\n        success = self.move_to_pose(approach_pose)\r\n        if not success:\r\n            return False\r\n\r\n        # 2. Move down to object\r\n        object_pose_stamped = PoseStamped()\r\n        object_pose_stamped.pose = object_pose\r\n        object_pose_stamped.header.frame_id = "base_link"\r\n\r\n        success = self.move_to_pose(object_pose_stamped)\r\n        if not success:\r\n            return False\r\n\r\n        # 3. Close gripper (publish gripper command)\r\n        # This would depend on your gripper interface\r\n        self.close_gripper()\r\n\r\n        # 4. Lift object\r\n        lift_pose = PoseStamped()\r\n        lift_pose.pose = object_pose\r\n        lift_pose.pose.position.z += lift_height\r\n        lift_pose.header.frame_id = "base_link"\r\n\r\n        success = self.move_to_pose(lift_pose)\r\n        return success\r\n\r\n    def place_object(self, target_pose, place_height=0.1):\r\n        """Execute place operation"""\r\n        # 1. Move to above placement position\r\n        approach_pose = PoseStamped()\r\n        approach_pose.pose = target_pose\r\n        approach_pose.pose.position.z += place_height\r\n        approach_pose.header.frame_id = "base_link"\r\n\r\n        success = self.move_to_pose(approach_pose)\r\n        if not success:\r\n            return False\r\n\r\n        # 2. Move down to placement position\r\n        place_pose_stamped = PoseStamped()\r\n        place_pose_stamped.pose = target_pose\r\n        place_pose_stamped.header.frame_id = "base_link"\r\n\r\n        success = self.move_to_pose(place_pose_stamped)\r\n        if not success:\r\n            return False\r\n\r\n        # 3. Open gripper\r\n        self.open_gripper()\r\n\r\n        # 4. Lift gripper\r\n        lift_pose = PoseStamped()\r\n        lift_pose.pose = target_pose\r\n        lift_pose.pose.position.z += place_height\r\n        lift_pose.header.frame_id = "base_link"\r\n\r\n        success = self.move_to_pose(lift_pose)\r\n        return success\r\n\r\n    def close_gripper(self):\r\n        """Close the gripper"""\r\n        # Implementation depends on gripper type\r\n        # This is a placeholder\r\n        self.get_logger().info("Closing gripper")\r\n\r\n    def open_gripper(self):\r\n        """Open the gripper"""\r\n        # Implementation depends on gripper type\r\n        # This is a placeholder\r\n        self.get_logger().info("Opening gripper")\r\n\r\n    def control_loop(self):\r\n        """Main control loop for manipulation"""\r\n        # This would contain the main manipulation logic\r\n        # For now, it just logs the current state\r\n        if self.joint_state:\r\n            self.get_logger().debug(f"Current joint positions: {self.joint_state.position[:3]}...")\r\n\r\nclass SimpleGripperController:\r\n    """Simple controller for parallel jaw gripper"""\r\n    def __init__(self, node, gripper_joint_name="gripper_joint"):\r\n        self.node = node\r\n        self.gripper_joint_name = gripper_joint_name\r\n        self.current_width = 0.0\r\n\r\n        # Publisher for gripper commands\r\n        self.gripper_pub = node.create_publisher(\r\n            JointTrajectory, \'/gripper_controller/joint_trajectory\', 10\r\n        )\r\n\r\n    def set_gripper_width(self, width):\r\n        """Set gripper width in meters"""\r\n        self.current_width = width\r\n\r\n        # Create joint trajectory for gripper\r\n        trajectory = JointTrajectory()\r\n        trajectory.joint_names = [self.gripper_joint_name]\r\n        trajectory.points = []\r\n\r\n        point = JointTrajectoryPoint()\r\n        point.positions = [width]  # This might need conversion depending on gripper type\r\n        point.velocities = [0.0]\r\n        point.accelerations = [0.0]\r\n        point.time_from_start.sec = 1  # 1 second to reach position\r\n        point.time_from_start.nanosec = 0\r\n\r\n        trajectory.points.append(point)\r\n        self.gripper_pub.publish(trajectory)\r\n\r\n    def open(self, width=0.08):\r\n        """Open gripper to specified width"""\r\n        self.set_gripper_width(width)\r\n\r\n    def close(self, width=0.01):\r\n        """Close gripper to specified width"""\r\n        self.set_gripper_width(width)\r\n\r\n    def grasp(self, object_width):\r\n        """Grasp an object of specified width"""\r\n        # Calculate appropriate gripper width\r\n        gripper_width = max(0.01, object_width - 0.01)  # Leave small gap\r\n        self.close(gripper_width)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"97-perception-action-integration",children:"9.7 Perception-Action Integration"}),"\n",(0,o.jsx)(n.h3,{id:"971-visual-servoing",children:"9.7.1 Visual Servoing"}),"\n",(0,o.jsx)(n.p,{children:"Visual servoing uses visual feedback to control robot motion:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class VisualServoController:\r\n    def __init__(self, camera_matrix, image_width=640, image_height=480):\r\n        self.camera_matrix = camera_matrix\r\n        self.image_width = image_width\r\n        self.image_height = image_height\r\n\r\n        # Control gains\r\n        self.kp = 1.0  # Proportional gain\r\n        self.ki = 0.1  # Integral gain\r\n        self.kd = 0.05  # Derivative gain\r\n\r\n        # Error history for integral and derivative terms\r\n        self.error_integral = np.zeros(2)\r\n        self.previous_error = np.zeros(2)\r\n        self.dt = 0.05  # Control loop time step\r\n\r\n    def compute_control(self, current_feature_pos, desired_feature_pos):\r\n        """\r\n        Compute control velocities based on feature position error\r\n        current_feature_pos: (u, v) pixel coordinates of feature in current image\r\n        desired_feature_pos: (u, v) pixel coordinates of desired feature position\r\n        """\r\n        # Calculate position error in image coordinates\r\n        error = np.array(desired_feature_pos) - np.array(current_feature_pos)\r\n\r\n        # Calculate proportional term\r\n        proportional = self.kp * error\r\n\r\n        # Calculate integral term\r\n        self.error_integral += error * self.dt\r\n        integral = self.ki * self.error_integral\r\n\r\n        # Calculate derivative term\r\n        derivative = self.kd * (error - self.previous_error) / self.dt if self.dt > 0 else np.zeros(2)\r\n\r\n        # Total control command\r\n        control_command = proportional + integral + derivative\r\n\r\n        # Store current error for next iteration\r\n        self.previous_error = error\r\n\r\n        # Convert image velocities to Cartesian velocities\r\n        # This requires the interaction matrix (Jacobian of image features)\r\n        # For simplicity, we\'ll return the image velocity directly\r\n        # In practice, you\'d convert this to end-effector velocities\r\n        cartesian_velocity = self.image_to_cartesian_velocity(control_command)\r\n\r\n        return cartesian_velocity\r\n\r\n    def image_to_cartesian_velocity(self, image_velocity):\r\n        """\r\n        Convert image velocity to Cartesian velocity\r\n        This is a simplified version - full implementation requires interaction matrix\r\n        """\r\n        # Simple approximation: assume constant depth\r\n        # In reality, you\'d need to compute the interaction matrix\r\n        z_depth = 1.0  # Assumed depth in meters\r\n\r\n        # Extract camera parameters\r\n        fx = self.camera_matrix[0, 0]\r\n        fy = self.camera_matrix[1, 1]\r\n        cx = self.camera_matrix[0, 2]\r\n        cy = self.camera_matrix[1, 2]\r\n\r\n        # Convert image velocity to angular velocity\r\n        # This is a simplified approximation\r\n        angular_velocity = np.zeros(6)  # [vx, vy, vz, wx, wy, wz]\r\n\r\n        # Approximate linear velocity components\r\n        angular_velocity[0] = -z_depth / fx * image_velocity[0]  # vx\r\n        angular_velocity[1] = -z_depth / fy * image_velocity[1]  # vy\r\n\r\n        return angular_velocity\r\n\r\n    def reset(self):\r\n        """Reset error integrals and derivatives"""\r\n        self.error_integral = np.zeros(2)\r\n        self.previous_error = np.zeros(2)\r\n\r\nclass FeatureTracker:\r\n    """Track visual features for visual servoing"""\r\n    def __init__(self):\r\n        import cv2\r\n        self.detector = cv2.SIFT_create()  # or ORB, AKAZE, etc.\r\n        self.matcher = cv2.BFMatcher()\r\n        self.previous_keypoints = None\r\n        self.previous_descriptors = None\r\n\r\n    def track_feature(self, current_image, template_image):\r\n        """Track a feature from template to current image"""\r\n        # Detect keypoints and descriptors\r\n        kp_current, desc_current = self.detector.detectAndCompute(current_image, None)\r\n        kp_template, desc_template = self.detector.detectAndCompute(template_image, None)\r\n\r\n        if desc_current is None or desc_template is None:\r\n            return None\r\n\r\n        # Match features\r\n        matches = self.matcher.knnMatch(desc_template, desc_current, k=2)\r\n\r\n        # Apply Lowe\'s ratio test\r\n        good_matches = []\r\n        for m, n in matches:\r\n            if m.distance < 0.75 * n.distance:\r\n                good_matches.append(m)\r\n\r\n        if len(good_matches) >= 4:\r\n            # Get matched keypoints\r\n            src_pts = np.float32([kp_template[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\r\n            dst_pts = np.float32([kp_current[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\r\n\r\n            # Calculate homography\r\n            homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\r\n\r\n            # Apply homography to template center to find current position\r\n            h, w = template_image.shape\r\n            template_center = np.float32([[w/2, h/2, 1]]).T\r\n            current_center = homography @ template_center\r\n            current_center = current_center / current_center[2]  # Normalize\r\n\r\n            return (int(current_center[0]), int(current_center[1]))\r\n\r\n        return None\n'})}),"\n",(0,o.jsx)(n.h2,{id:"98-safety-and-robustness",children:"9.8 Safety and Robustness"}),"\n",(0,o.jsx)(n.h3,{id:"981-force-control-and-compliance",children:"9.8.1 Force Control and Compliance"}),"\n",(0,o.jsx)(n.p,{children:"Force control allows robots to be compliant and safe when interacting with the environment:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ForceController:\r\n    def __init__(self, desired_force, stiffness=1000, damping=10):\r\n        self.desired_force = desired_force\r\n        self.stiffness = stiffness\r\n        self.damping = damping\r\n\r\n        # Force PID controller\r\n        self.force_pid = PIDController(kp=1.0, ki=0.1, kd=0.05)\r\n\r\n        # Previous values for derivative calculation\r\n        self.previous_force_error = 0\r\n        self.force_integral = 0\r\n\r\n    def compute_compliance_motion(self, current_force, current_position, dt=0.01):\r\n        """Compute motion based on force feedback"""\r\n        # Calculate force error\r\n        force_error = self.desired_force - current_force\r\n\r\n        # Use PID to calculate position adjustment\r\n        position_adjustment = self.force_pid.compute(force_error, 0)  # 0 is measured value (we want error = 0)\r\n\r\n        # Calculate desired position\r\n        desired_position = current_position + position_adjustment * dt\r\n\r\n        return desired_position\r\n\r\n    def impedance_control(self, desired_position, current_position, current_velocity,\r\n                         external_force, dt=0.01):\r\n        """Implement impedance control"""\r\n        # Calculate position and velocity errors\r\n        pos_error = desired_position - current_position\r\n        vel_error = -current_velocity  # Assuming desired velocity is 0\r\n\r\n        # Calculate impedance forces\r\n        spring_force = self.stiffness * pos_error\r\n        damper_force = self.damping * vel_error\r\n\r\n        # Total force\r\n        total_force = spring_force + damper_force + external_force\r\n\r\n        # Calculate acceleration (F = ma, so a = F/m)\r\n        # For simplicity, assume unit mass\r\n        acceleration = total_force\r\n\r\n        # Update velocity and position\r\n        new_velocity = current_velocity + acceleration * dt\r\n        new_position = current_position + new_velocity * dt\r\n\r\n        return new_position, new_velocity\r\n\r\nclass SafetyMonitor:\r\n    """Monitor manipulation for safety"""\r\n    def __init__(self):\r\n        self.max_force_threshold = 50.0  # Newtons\r\n        self.max_velocity_threshold = 1.0  # m/s\r\n        self.collision_threshold = 0.05  # meters to obstacles\r\n        self.emergency_stop = False\r\n\r\n    def check_safety(self, current_forces, current_velocities, proximity_sensors):\r\n        """Check if current state is safe"""\r\n        # Check force limits\r\n        if any(abs(f) > self.max_force_threshold for f in current_forces):\r\n            self.emergency_stop = True\r\n            return False, "Force limit exceeded"\r\n\r\n        # Check velocity limits\r\n        if any(abs(v) > self.max_velocity_threshold for v in current_velocities):\r\n            self.emergency_stop = True\r\n            return False, "Velocity limit exceeded"\r\n\r\n        # Check proximity to obstacles\r\n        if any(dist < self.collision_threshold for dist in proximity_sensors):\r\n            self.emergency_stop = True\r\n            return False, "Collision imminent"\r\n\r\n        return True, "Safe"\r\n\r\n    def reset_safety(self):\r\n        """Reset safety monitor"""\r\n        self.emergency_stop = False\n'})}),"\n",(0,o.jsx)(n.h2,{id:"99-integration-with-physical-ai-systems",children:"9.9 Integration with Physical AI Systems"}),"\n",(0,o.jsx)(n.p,{children:"Manipulation systems in Physical AI must seamlessly integrate perception, planning, and control:"}),"\n",(0,o.jsx)(n.h3,{id:"991-perception-action-loop-for-manipulation",children:"9.9.1 Perception-Action Loop for Manipulation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class IntegratedManipulationSystem:\r\n    def __init__(self):\r\n        # Initialize all components\r\n        self.perception_module = PerceptionModule()\r\n        self.grasp_planner = GraspPlanner(None, None)\r\n        self.motion_planner = TaskAndMotionPlanner(None, {})\r\n        self.controller = ManipulationControllerNode()\r\n        self.visual_servo = VisualServoController(None)\r\n        self.safety_monitor = SafetyMonitor()\r\n\r\n        # Task queue\r\n        self.task_queue = []\r\n        self.current_task = None\r\n\r\n    def execute_manipulation_task(self, task_description):\r\n        """Execute a complete manipulation task"""\r\n        # 1. Perceive the environment\r\n        objects = self.perception_module.detect_objects()\r\n\r\n        # 2. Plan grasps for target object\r\n        target_object = self.find_target_object(task_description, objects)\r\n        if not target_object:\r\n            return False, "Target object not found"\r\n\r\n        grasps = self.grasp_planner.plan_grasps(\r\n            target_object[\'mesh\'],\r\n            target_object[\'pose\']\r\n        )\r\n\r\n        # 3. Execute manipulation with best grasp\r\n        for grasp in grasps:\r\n            success = self.execute_grasp(grasp, target_object)\r\n            if success:\r\n                return True, "Successfully executed manipulation task"\r\n\r\n        return False, "No feasible grasp found"\r\n\r\n    def execute_grasp(self, grasp, target_object):\r\n        """Execute a specific grasp"""\r\n        # Check safety before execution\r\n        is_safe, reason = self.safety_monitor.check_safety(\r\n            [0, 0, 0], [0, 0, 0], [1.0, 1.0, 1.0]\r\n        )\r\n\r\n        if not is_safe:\r\n            return False\r\n\r\n        # Move to approach pose\r\n        approach_pose = self.calculate_approach_pose(grasp)\r\n        success = self.controller.move_to_pose(approach_pose)\r\n        if not success:\r\n            return False\r\n\r\n        # Use visual servoing to fine-tune position if needed\r\n        if self.needs_fine_alignment(target_object):\r\n            self.visual_servo_alignment(target_object, grasp)\r\n\r\n        # Execute grasp\r\n        self.execute_approach_and_grasp(grasp)\r\n\r\n        # Lift object\r\n        lift_pose = self.calculate_lift_pose(grasp)\r\n        success = self.controller.move_to_pose(lift_pose)\r\n\r\n        return success\r\n\r\n    def calculate_approach_pose(self, grasp, distance=0.1):\r\n        """Calculate approach pose before grasp"""\r\n        grasp_transform = grasp.to_transformation_matrix()\r\n        approach_transform = grasp_transform.copy()\r\n\r\n        # Move back along approach direction\r\n        approach_direction = grasp.approach_direction\r\n        approach_transform[0:3, 3] += distance * np.array(approach_direction)\r\n\r\n        return approach_transform\r\n\r\n    def calculate_lift_pose(self, grasp, height=0.1):\r\n        """Calculate lift pose after grasp"""\r\n        grasp_transform = grasp.to_transformation_matrix()\r\n        lift_transform = grasp_transform.copy()\r\n\r\n        # Move up along z-axis\r\n        lift_transform[0:3, 3][2] += height\r\n\r\n        return lift_transform\r\n\r\n    def needs_fine_alignment(self, target_object):\r\n        """Determine if fine alignment is needed"""\r\n        # This would be based on object properties and task requirements\r\n        return target_object.get(\'precision_required\', False)\r\n\r\n    def visual_servo_alignment(self, target_object, grasp):\r\n        """Use visual servoing for fine alignment"""\r\n        # Get current camera image\r\n        current_image = self.perception_module.get_current_image()\r\n\r\n        # Track target feature\r\n        target_feature_pos = self.get_target_feature_position(target_object)\r\n        current_feature_pos = self.visual_servo.track_feature(\r\n            current_image, target_feature_pos\r\n        )\r\n\r\n        # Compute control to align features\r\n        if current_feature_pos is not None:\r\n            velocity = self.visual_servo.compute_control(\r\n                current_feature_pos, target_feature_pos\r\n            )\r\n\r\n            # Apply small adjustment to approach pose\r\n            self.apply_velocity_correction(velocity)\r\n\r\n    def execute_approach_and_grasp(self, grasp):\r\n        """Execute the approach and grasp motion"""\r\n        # Move to grasp pose\r\n        grasp_pose = grasp.to_transformation_matrix()\r\n        self.controller.move_to_pose(grasp_pose)\r\n\r\n        # Close gripper\r\n        self.controller.close_gripper()\r\n\r\n    def find_target_object(self, task_description, detected_objects):\r\n        """Find the target object based on task description"""\r\n        for obj in detected_objects:\r\n            if task_description[\'object_name\'] in obj[\'name\']:\r\n                return obj\r\n        return None\r\n\r\n    def apply_velocity_correction(self, velocity):\r\n        """Apply small velocity corrections during manipulation"""\r\n        # This would send small velocity commands to adjust position\r\n        # Implementation depends on the robot\'s control interface\r\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Robotic manipulation and control systems form the core of dexterous robot capabilities. The integration of kinematics, dynamics, control theory, and perception enables robots to interact with their environment in meaningful ways. Modern manipulation systems leverage advanced control techniques like operational space control and impedance control to achieve safe and robust interaction. The combination of perception, planning, and control in a unified framework allows robots to perform complex manipulation tasks in unstructured environments. As Physical AI systems continue to evolve, manipulation capabilities will become increasingly sophisticated, enabling robots to perform human-like dexterous tasks safely and efficiently."})]})}function _(r={}){const{wrapper:n}={...(0,a.R)(),...r.components};return n?(0,o.jsx)(n,{...r,children:(0,o.jsx)(c,{...r})}):c(r)}},8453:(r,n,e)=>{e.d(n,{R:()=>i,x:()=>s});var t=e(6540);const o={},a=t.createContext(o);function i(r){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof r?r(n):{...n,...r}},[n,r])}function s(r){let n;return n=r.disableParentContext?"function"==typeof r.components?r.components(o):r.components||o:i(r.components),t.createElement(a.Provider,{value:n},r.children)}}}]);