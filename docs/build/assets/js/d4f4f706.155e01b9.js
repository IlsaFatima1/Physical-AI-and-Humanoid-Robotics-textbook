"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[354],{8155:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"ch07-perception-systems/exercises/chapter7-quiz","title":"Chapter 7 Quiz: Perception Systems and Computer Vision","description":"Multiple Choice Questions","source":"@site/ch07-perception-systems/exercises/chapter7-quiz.md","sourceDirName":"ch07-perception-systems/exercises","slug":"/ch07-perception-systems/exercises/chapter7-quiz","permalink":"/ch07-perception-systems/exercises/chapter7-quiz","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/ch07-perception-systems/exercises/chapter7-quiz.md","tags":[],"version":"current","frontMatter":{}}');var t=i(4848),r=i(8453);const o={},a="Chapter 7 Quiz: Perception Systems and Computer Vision",c={},l=[{value:"Multiple Choice Questions",id:"multiple-choice-questions",level:2},{value:"Practical Application Questions",id:"practical-application-questions",level:2},{value:"Code Analysis Questions",id:"code-analysis-questions",level:2},{value:"Conceptual Questions",id:"conceptual-questions",level:2},{value:"Answer Key",id:"answer-key",level:2},{value:"Multiple Choice Answers:",id:"multiple-choice-answers",level:3},{value:"Practical Application Answers:",id:"practical-application-answers",level:3},{value:"Code Analysis Answers:",id:"code-analysis-answers",level:3},{value:"Conceptual Answers:",id:"conceptual-answers",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-7-quiz-perception-systems-and-computer-vision",children:"Chapter 7 Quiz: Perception Systems and Computer Vision"})}),"\n",(0,t.jsx)(n.h2,{id:"multiple-choice-questions",children:"Multiple Choice Questions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is the primary purpose of camera calibration in robotics perception?\r\na) To improve image resolution\r\nb) To correct lens distortion and establish the relationship between pixel and world coordinates\r\nc) To increase frame rate\r\nd) To reduce image noise"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Which of the following is NOT a typical component of the perception-action loop?\r\na) Sensing\r\nb) Processing\r\nc) Memorizing\r\nd) Acting"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What does IoU (Intersection over Union) measure in object detection?\r\na) Image brightness\r\nb) The overlap between predicted and ground truth bounding boxes\r\nc) Camera focal length\r\nd) Processing speed"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Which deep learning architecture is commonly used for semantic segmentation?\r\na) ResNet\r\nb) U-Net\r\nc) LSTM\r\nd) GRU"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is the main advantage of stereo vision over monocular vision?\r\na) Higher resolution\r\nb) Lower computational cost\r\nc) Direct depth estimation\r\nd) Better color representation"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practical-application-questions",children:"Practical Application Questions"}),"\n",(0,t.jsxs)(n.ol,{start:"6",children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"You are developing a perception system for a mobile robot that needs to detect and avoid obstacles in real-time. Design a complete pipeline that includes:\r\na) Preprocessing steps to handle varying lighting conditions\r\nb) Object detection approach (traditional vs. deep learning)\r\nc) How to integrate the perception output with navigation planning\r\nd) Methods to handle uncertainty in detections"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Implement a multi-sensor fusion approach that combines camera and LiDAR data for robust object detection. Describe:\r\na) How to align the coordinate systems of different sensors\r\nb) The fusion algorithm you would use\r\nc) How to handle temporal synchronization issues\r\nd) Validation methods for the fused output"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Design a semantic segmentation system for an autonomous robot that operates in indoor environments. Consider:\r\na) Which classes to include in the segmentation\r\nb) Network architecture choices\r\nc) Training data requirements\r\nd) Real-time performance optimization"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"code-analysis-questions",children:"Code Analysis Questions"}),"\n",(0,t.jsxs)(n.ol,{start:"9",children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Analyze the following object detection code and identify potential issues:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def detect_objects(image):\r\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n    # Simple thresholding approach\r\n    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\r\n\r\n    # Find contours\r\n    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n    objects = []\r\n    for contour in contours:\r\n        area = cv2.contourArea(contour)\r\n        if area > 50:  # Fixed threshold\r\n            x, y, w, h = cv2.boundingRect(contour)\r\n            objects.append({'bbox': (x, y, w, h)})\r\n\r\n    return objects\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The following ROS 2 perception node has potential performance issues. Identify and suggest improvements:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class PerceptionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('perception_node')\r\n        self.cv_bridge = CvBridge()\r\n\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/image_raw', self.image_callback, 1\r\n        )\r\n\r\n    def image_callback(self, msg):\r\n        # Process image synchronously (blocking)\r\n        cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n\r\n        # Heavy processing that blocks the callback\r\n        results = self.heavy_processing(cv_image)\r\n\r\n        # Publish results\r\n        self.publish_results(results)\r\n\r\n        # No rate limiting - processes every image\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conceptual-questions",children:"Conceptual Questions"}),"\n",(0,t.jsxs)(n.ol,{start:"11",children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Explain the differences between semantic segmentation, instance segmentation, and panoptic segmentation. When would you use each approach in robotics applications?"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Describe the challenges of performing perception in dynamic environments and propose solutions to handle moving objects, changing lighting, and camera motion."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"How does uncertainty quantification improve the safety and reliability of robotic perception systems? What methods would you use to estimate uncertainty?"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Discuss the trade-offs between traditional computer vision methods and deep learning approaches for robotics perception. When would you choose one over the other?"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"answer-key",children:"Answer Key"}),"\n",(0,t.jsx)(n.h3,{id:"multiple-choice-answers",children:"Multiple Choice Answers:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"b) To correct lens distortion and establish the relationship between pixel and world coordinates"}),"\n",(0,t.jsx)(n.li,{children:"c) Memorizing"}),"\n",(0,t.jsx)(n.li,{children:"b) The overlap between predicted and ground truth bounding boxes"}),"\n",(0,t.jsx)(n.li,{children:"b) U-Net"}),"\n",(0,t.jsx)(n.li,{children:"c) Direct depth estimation"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"practical-application-answers",children:"Practical Application Answers:"}),"\n",(0,t.jsxs)(n.ol,{start:"6",children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Real-time obstacle detection pipeline:\r\na) Preprocessing: Histogram equalization, adaptive thresholding, noise reduction\r\nb) Approach: Deep learning (YOLO or SSD) for accuracy, traditional methods as fallback\r\nc) Integration: Publish detections to costmap_2d for navigation stack\r\nd) Uncertainty: Confidence thresholds, temporal consistency checks, sensor fusion"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Multi-sensor fusion approach:\r\na) Alignment: Extrinsic calibration to find transformation matrix between sensors\r\nb) Algorithm: Probabilistic fusion using Kalman filters or particle filters\r\nc) Synchronization: Timestamp interpolation and buffer management\r\nd) Validation: Cross-validation between sensors, consistency checks"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Semantic segmentation system:\r\na) Classes: Walls, floor, furniture, people, robots, obstacles\r\nb) Architecture: Efficient encoder-decoder like MobileNet + U-Net for real-time\r\nc) Data: Synthetic data generation + real-world annotated images\r\nd) Optimization: Model quantization, pruning, and hardware acceleration"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"code-analysis-answers",children:"Code Analysis Answers:"}),"\n",(0,t.jsxs)(n.ol,{start:"9",children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Issues with the object detection code:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Fixed threshold doesn't adapt to lighting conditions"}),"\n",(0,t.jsx)(n.li,{children:"No noise filtering before thresholding"}),"\n",(0,t.jsx)(n.li,{children:"Fixed area threshold may not work for different object sizes"}),"\n",(0,t.jsx)(n.li,{children:"No validation of geometric properties"}),"\n",(0,t.jsx)(n.li,{children:"Improvements: Adaptive thresholding, morphological operations, aspect ratio checks"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Issues with the ROS 2 node:"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Heavy processing in callback blocks message handling"}),"\n",(0,t.jsx)(n.li,{children:"No rate limiting causes excessive processing"}),"\n",(0,t.jsx)(n.li,{children:"Fixed queue size of 1 may drop messages"}),"\n",(0,t.jsx)(n.li,{children:"Improvements: Use separate processing thread, rate limiting, larger queue size:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def image_callback(self, msg):\r\n    # Just store image for processing\r\n    self.latest_image = msg\r\n\r\ndef processing_timer_callback(self):\r\n    if self.latest_image is not None:\r\n        # Process in separate thread or use asyncio\r\n        self.process_image_async(self.latest_image)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"conceptual-answers",children:"Conceptual Answers:"}),"\n",(0,t.jsxs)(n.ol,{start:"11",children:["\n",(0,t.jsx)(n.li,{children:"Segmentation differences:"}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Semantic segmentation: Pixel-level classification without instance separation"}),"\n",(0,t.jsx)(n.li,{children:"Instance segmentation: Distinguishes between different instances of same class"}),"\n",(0,t.jsx)(n.li,{children:"Panoptic segmentation: Combines semantic and instance segmentation\r\nUse semantic for scene understanding, instance for object manipulation, panoptic for comprehensive scene analysis"}),"\n"]}),"\n",(0,t.jsxs)(n.ol,{start:"12",children:["\n",(0,t.jsx)(n.li,{children:"Dynamic environment challenges:"}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Moving objects: Background subtraction, optical flow, temporal differencing"}),"\n",(0,t.jsx)(n.li,{children:"Changing lighting: Adaptive algorithms, image enhancement, multiple models"}),"\n",(0,t.jsx)(n.li,{children:"Camera motion: Motion compensation, visual-inertial fusion, motion blur reduction"}),"\n"]}),"\n",(0,t.jsxs)(n.ol,{start:"13",children:["\n",(0,t.jsx)(n.li,{children:"Uncertainty quantification benefits:"}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Enables safe decision-making under uncertainty"}),"\n",(0,t.jsx)(n.li,{children:"Helps determine when to request human intervention"}),"\n",(0,t.jsx)(n.li,{children:"Methods: Monte Carlo dropout, ensemble methods, Bayesian approaches"}),"\n",(0,t.jsx)(n.li,{children:"Allows dynamic confidence threshold adjustment"}),"\n"]}),"\n",(0,t.jsxs)(n.ol,{start:"14",children:["\n",(0,t.jsx)(n.li,{children:"Traditional vs. deep learning trade-offs:"}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Traditional: Interpretable, less data required, faster inference, limited complexity"}),"\n",(0,t.jsx)(n.li,{children:"Deep learning: Better accuracy on complex tasks, requires large datasets, computationally intensive"}),"\n",(0,t.jsx)(n.li,{children:"Choose traditional for simple, well-defined tasks; deep learning for complex perception"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);